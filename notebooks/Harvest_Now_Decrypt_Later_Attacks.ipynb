{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa1b137",
   "metadata": {},
   "source": [
    "#  Harvest Now, Decrypt Later (HNDL) Attacks\n",
    "\n",
    "**Author:** Mauro Risonho de Paula Assumpção aka firebitsbr  \n",
    "**License:** MIT  \n",
    "**Date:** August 7, 2025\n",
    "\n",
    "##  Overview\n",
    "\n",
    "The \"Harvest Now, Decrypt Later\" strategy involves collecting encrypted data today for future decryption when quantum computers become capable enough. This notebook demonstrates:\n",
    "\n",
    "-  **TLS traffic capture** and storage strategies\n",
    "-  **Future vulnerability assessment** of captured data\n",
    "- ⏰ **Post-quantum migration urgency** analysis\n",
    "-  **Economic impact** of delayed decryption\n",
    "\n",
    "###  **Legal Disclaimer**\n",
    "\n",
    "This notebook is for **authorized security testing and educational purposes only**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a5508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Environment Setup and Imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add Houdinis to path\n",
    "sys.path.append('/home/test/Downloads/Projetos/Houdinis')\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import json\n",
    "import base64\n",
    "import sqlite3\n",
    "\n",
    "# Houdinis framework imports\n",
    "try:\n",
    "    from quantum.backend import QuantumBackendManager\n",
    "    from payloads.decrypt_tls import TLSDecryptor\n",
    "    print(\" Houdinis modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\" Houdinis import error: {e}\")\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "print(\" Environment setup complete!\")\n",
    "print(f\" Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  HNDL Attack Simulator\n",
    "class HNDLSimulator:\n",
    "    \"\"\"Simulate harvest now decrypt later attack scenarios\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.captured_data = []\n",
    "        self.vulnerability_timeline = {\n",
    "            'RSA-1024': {'broken': 2025, 'risk_level': 'CRITICAL'},\n",
    "            'RSA-2048': {'broken': 2030, 'risk_level': 'HIGH'}, \n",
    "            'RSA-4096': {'broken': 2035, 'risk_level': 'MEDIUM'},\n",
    "            'ECDH-256': {'broken': 2028, 'risk_level': 'HIGH'},\n",
    "            'ECDH-384': {'broken': 2032, 'risk_level': 'MEDIUM'},\n",
    "            'DH-2048': {'broken': 2030, 'risk_level': 'HIGH'},\n",
    "            'DH-3072': {'broken': 2035, 'risk_level': 'MEDIUM'}\n",
    "        }\n",
    "    \n",
    "    def simulate_tls_capture(self, num_sessions=100):\n",
    "        \"\"\"Simulate TLS traffic capture\"\"\"\n",
    "        print(\" Simulating TLS traffic interception...\")\n",
    "        \n",
    "        algorithms = ['RSA-2048', 'ECDH-256', 'RSA-1024', 'ECDH-384', 'DH-2048', 'RSA-4096']\n",
    "        data_types = ['financial', 'medical', 'government', 'corporate', 'personal', 'research']\n",
    "        source_countries = ['US', 'CN', 'RU', 'DE', 'GB', 'FR', 'JP', 'IN', 'BR', 'CA']\n",
    "        \n",
    "        for i in range(num_sessions):\n",
    "            capture_date = datetime.now() - timedelta(days=random.randint(0, 1095))  # Last 3 years\n",
    "            algorithm = random.choice(algorithms)\n",
    "            \n",
    "            session = {\n",
    "                'session_id': f\"TLS_{i:06d}\",\n",
    "                'timestamp': capture_date,\n",
    "                'algorithm': algorithm,\n",
    "                'data_type': random.choice(data_types),\n",
    "                'data_size': random.randint(1024, 10485760),  # 1KB to 10MB\n",
    "                'encrypted_payload': base64.b64encode(os.urandom(64)).decode(),\n",
    "                'source_country': random.choice(source_countries),\n",
    "                'dest_country': random.choice(source_countries),\n",
    "                'risk_score': self.calculate_risk_score(algorithm, capture_date),\n",
    "                'value_score': self.calculate_value_score(random.choice(data_types)),\n",
    "                'urgency_score': self.calculate_urgency_score(algorithm, capture_date)\n",
    "            }\n",
    "            self.captured_data.append(session)\n",
    "        \n",
    "        print(f\" Captured {len(self.captured_data)} TLS sessions\")\n",
    "        return self.captured_data\n",
    "    \n",
    "    def calculate_risk_score(self, algorithm, capture_date):\n",
    "        \"\"\"Calculate quantum vulnerability risk score\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        capture_year = capture_date.year\n",
    "        \n",
    "        if algorithm in self.vulnerability_timeline:\n",
    "            break_year = self.vulnerability_timeline[algorithm]['broken']\n",
    "            years_to_break = break_year - current_year\n",
    "            data_age = current_year - capture_year\n",
    "            \n",
    "            # Higher score for data captured longer ago and algorithms breaking sooner\n",
    "            risk = 10 - years_to_break + (data_age * 0.5)\n",
    "            return max(1, min(10, risk))\n",
    "        return 3.0\n",
    "    \n",
    "    def calculate_value_score(self, data_type):\n",
    "        \"\"\"Calculate the intelligence/economic value of data type\"\"\"\n",
    "        value_map = {\n",
    "            'government': 10,\n",
    "            'military': 10,\n",
    "            'financial': 9,\n",
    "            'medical': 8,\n",
    "            'research': 7,\n",
    "            'corporate': 6,\n",
    "            'personal': 4\n",
    "        }\n",
    "        return value_map.get(data_type, 5) + random.uniform(-1, 1)\n",
    "    \n",
    "    def calculate_urgency_score(self, algorithm, capture_date):\n",
    "        \"\"\"Calculate urgency for decryption based on data age and algorithm timeline\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        data_age = current_year - capture_date.year\n",
    "        \n",
    "        if algorithm in self.vulnerability_timeline:\n",
    "            break_year = self.vulnerability_timeline[algorithm]['broken']\n",
    "            years_to_break = break_year - current_year\n",
    "            \n",
    "            # More urgent if breaking soon and data is recent\n",
    "            urgency = (10 - years_to_break) + max(0, 5 - data_age)\n",
    "            return max(1, min(10, urgency))\n",
    "        return 5.0\n",
    "\n",
    "# Initialize HNDL simulator\n",
    "print(\" HNDL Attack Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "hndl = HNDLSimulator()\n",
    "\n",
    "# Simulate traffic capture over multiple years\n",
    "captured_sessions = hndl.simulate_tls_capture(500)\n",
    "\n",
    "print(f\"\\n Initial Capture Statistics:\")\n",
    "print(f\"  • Total sessions: {len(captured_sessions)}\")\n",
    "print(f\"  • Time range: {min(s['timestamp'] for s in captured_sessions).strftime('%Y-%m-%d')} to {max(s['timestamp'] for s in captured_sessions).strftime('%Y-%m-%d')}\")\n",
    "print(f\"  • Total data volume: {sum(s['data_size'] for s in captured_sessions) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e79832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  HNDL Data Analysis\n",
    "def analyze_hndl_data(sessions):\n",
    "    \"\"\"Comprehensive analysis of harvested data\"\"\"\n",
    "    \n",
    "    analysis = {\n",
    "        'total_sessions': len(sessions),\n",
    "        'algorithm_distribution': {},\n",
    "        'data_type_distribution': {},\n",
    "        'geographic_distribution': {},\n",
    "        'risk_analysis': {},\n",
    "        'timeline_analysis': {},\n",
    "        'value_assessment': {}\n",
    "    }\n",
    "    \n",
    "    # Algorithm distribution\n",
    "    for session in sessions:\n",
    "        algo = session['algorithm']\n",
    "        if algo not in analysis['algorithm_distribution']:\n",
    "            analysis['algorithm_distribution'][algo] = {'count': 0, 'total_size': 0, 'avg_risk': 0}\n",
    "        analysis['algorithm_distribution'][algo]['count'] += 1\n",
    "        analysis['algorithm_distribution'][algo]['total_size'] += session['data_size']\n",
    "        analysis['algorithm_distribution'][algo]['avg_risk'] += session['risk_score']\n",
    "    \n",
    "    # Calculate averages\n",
    "    for algo_data in analysis['algorithm_distribution'].values():\n",
    "        algo_data['avg_risk'] /= algo_data['count']\n",
    "        algo_data['total_size_mb'] = algo_data['total_size'] / (1024 * 1024)\n",
    "    \n",
    "    # Data type analysis\n",
    "    for session in sessions:\n",
    "        data_type = session['data_type']\n",
    "        if data_type not in analysis['data_type_distribution']:\n",
    "            analysis['data_type_distribution'][data_type] = {\n",
    "                'count': 0, 'avg_value': 0, 'avg_risk': 0, 'total_size': 0\n",
    "            }\n",
    "        analysis['data_type_distribution'][data_type]['count'] += 1\n",
    "        analysis['data_type_distribution'][data_type]['avg_value'] += session['value_score']\n",
    "        analysis['data_type_distribution'][data_type]['avg_risk'] += session['risk_score']\n",
    "        analysis['data_type_distribution'][data_type]['total_size'] += session['data_size']\n",
    "    \n",
    "    # Calculate data type averages\n",
    "    for dt_data in analysis['data_type_distribution'].values():\n",
    "        dt_data['avg_value'] /= dt_data['count']\n",
    "        dt_data['avg_risk'] /= dt_data['count']\n",
    "        dt_data['total_size_mb'] = dt_data['total_size'] / (1024 * 1024)\n",
    "    \n",
    "    # Geographic analysis\n",
    "    for session in sessions:\n",
    "        country_pair = f\"{session['source_country']}->{session['dest_country']}\"\n",
    "        if country_pair not in analysis['geographic_distribution']:\n",
    "            analysis['geographic_distribution'][country_pair] = 0\n",
    "        analysis['geographic_distribution'][country_pair] += 1\n",
    "    \n",
    "    # Risk categorization\n",
    "    risk_categories = {'Critical (9-10)': 0, 'High (7-8)': 0, 'Medium (5-6)': 0, 'Low (1-4)': 0}\n",
    "    for session in sessions:\n",
    "        risk = session['risk_score']\n",
    "        if risk >= 9:\n",
    "            risk_categories['Critical (9-10)'] += 1\n",
    "        elif risk >= 7:\n",
    "            risk_categories['High (7-8)'] += 1\n",
    "        elif risk >= 5:\n",
    "            risk_categories['Medium (5-6)'] += 1\n",
    "        else:\n",
    "            risk_categories['Low (1-4)'] += 1\n",
    "    \n",
    "    analysis['risk_analysis'] = risk_categories\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Perform comprehensive analysis\n",
    "analysis = analyze_hndl_data(captured_sessions)\n",
    "\n",
    "print(\" HNDL Data Analysis Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n Algorithm Vulnerability Breakdown:\")\n",
    "for algo, data in analysis['algorithm_distribution'].items():\n",
    "    risk_info = hndl.vulnerability_timeline.get(algo, {'broken': 'Unknown', 'risk_level': 'UNKNOWN'})\n",
    "    print(f\"  • {algo}: {data['count']} sessions, {data['total_size_mb']:.1f} MB\")\n",
    "    print(f\"     Break year: {risk_info['broken']}, Risk: {risk_info['risk_level']}, Avg score: {data['avg_risk']:.1f}\")\n",
    "\n",
    "print(f\"\\n Data Type Intelligence Value:\")\n",
    "sorted_data_types = sorted(analysis['data_type_distribution'].items(), \n",
    "                          key=lambda x: x[1]['avg_value'], reverse=True)\n",
    "for data_type, data in sorted_data_types:\n",
    "    print(f\"  • {data_type.capitalize()}: {data['count']} sessions, Value: {data['avg_value']:.1f}/10, Risk: {data['avg_risk']:.1f}/10\")\n",
    "\n",
    "print(f\"\\n  Risk Distribution:\")\n",
    "for risk_cat, count in analysis['risk_analysis'].items():\n",
    "    percentage = (count / analysis['total_sessions']) * 100\n",
    "    print(f\"  • {risk_cat}: {count} sessions ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  HNDL Attack Visualization\n",
    "print(\" Creating HNDL attack visualizations...\")\n",
    "\n",
    "# Create comprehensive visualization dashboard\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Algorithm distribution and risk\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "algos = list(analysis['algorithm_distribution'].keys())\n",
    "counts = [analysis['algorithm_distribution'][algo]['count'] for algo in algos]\n",
    "risks = [analysis['algorithm_distribution'][algo]['avg_risk'] for algo in algos]\n",
    "colors = ['red' if r >= 8 else 'orange' if r >= 6 else 'yellow' if r >= 4 else 'green' for r in risks]\n",
    "\n",
    "bars = ax1.bar(algos, counts, color=colors, alpha=0.7)\n",
    "ax1.set_title('Captured Sessions by Algorithm')\n",
    "ax1.set_ylabel('Number of Sessions')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Data type value assessment\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "data_types = list(analysis['data_type_distribution'].keys())\n",
    "values = [analysis['data_type_distribution'][dt]['avg_value'] for dt in data_types]\n",
    "dt_counts = [analysis['data_type_distribution'][dt]['count'] for dt in data_types]\n",
    "\n",
    "bubble_sizes = [c * 20 for c in dt_counts]  # Scale for visibility\n",
    "scatter = ax2.scatter(range(len(data_types)), values, s=bubble_sizes, alpha=0.6, c=values, cmap='Reds')\n",
    "ax2.set_title('Data Type Intelligence Value')\n",
    "ax2.set_ylabel('Intelligence Value Score')\n",
    "ax2.set_xticks(range(len(data_types)))\n",
    "ax2.set_xticklabels(data_types, rotation=45)\n",
    "plt.colorbar(scatter, ax=ax2, label='Value Score')\n",
    "\n",
    "# Risk distribution pie chart\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "risk_labels = list(analysis['risk_analysis'].keys())\n",
    "risk_values = list(analysis['risk_analysis'].values())\n",
    "risk_colors = ['darkred', 'red', 'orange', 'lightgreen']\n",
    "\n",
    "ax3.pie(risk_values, labels=risk_labels, colors=risk_colors, autopct='%1.1f%%', startangle=90)\n",
    "ax3.set_title('Risk Level Distribution')\n",
    "\n",
    "# Timeline analysis\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "# Create timeline data\n",
    "timeline_data = {}\n",
    "for session in captured_sessions:\n",
    "    year = session['timestamp'].year\n",
    "    if year not in timeline_data:\n",
    "        timeline_data[year] = {'count': 0, 'total_risk': 0}\n",
    "    timeline_data[year]['count'] += 1\n",
    "    timeline_data[year]['total_risk'] += session['risk_score']\n",
    "\n",
    "years = sorted(timeline_data.keys())\n",
    "yearly_counts = [timeline_data[year]['count'] for year in years]\n",
    "yearly_avg_risk = [timeline_data[year]['total_risk'] / timeline_data[year]['count'] for year in years]\n",
    "\n",
    "ax4_twin = ax4.twinx()\n",
    "bars = ax4.bar(years, yearly_counts, alpha=0.7, color='lightblue', label='Sessions Captured')\n",
    "line = ax4_twin.plot(years, yearly_avg_risk, 'ro-', linewidth=2, markersize=6, label='Avg Risk Score')\n",
    "\n",
    "ax4.set_xlabel('Year')\n",
    "ax4.set_ylabel('Sessions Captured', color='blue')\n",
    "ax4_twin.set_ylabel('Average Risk Score', color='red')\n",
    "ax4.set_title('Capture Timeline vs Risk Evolution')\n",
    "ax4.legend(loc='upper left')\n",
    "ax4_twin.legend(loc='upper right')\n",
    "\n",
    "# Quantum threat timeline\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "threat_years = list(range(2024, 2041))\n",
    "algorithms_timeline = ['RSA-1024', 'RSA-2048', 'ECDH-256', 'DH-2048', 'RSA-4096']\n",
    "threat_colors = ['red', 'orange', 'blue', 'green', 'purple']\n",
    "\n",
    "for i, algo in enumerate(algorithms_timeline):\n",
    "    if algo in hndl.vulnerability_timeline:\n",
    "        break_year = hndl.vulnerability_timeline[algo]['broken']\n",
    "        sessions_count = analysis['algorithm_distribution'].get(algo, {}).get('count', 0)\n",
    "        \n",
    "        # Create threat progression\n",
    "        threat_level = []\n",
    "        for year in threat_years:\n",
    "            if year < break_year:\n",
    "                threat_level.append(sessions_count * (year - 2024) / (break_year - 2024))\n",
    "            else:\n",
    "                threat_level.append(sessions_count)\n",
    "        \n",
    "        ax5.plot(threat_years, threat_level, color=threat_colors[i], linewidth=2, \n",
    "                marker='o', markersize=4, label=f'{algo} ({sessions_count} sessions)')\n",
    "\n",
    "ax5.set_xlabel('Year')\n",
    "ax5.set_ylabel('Vulnerable Sessions')\n",
    "ax5.set_title('Quantum Threat Timeline')\n",
    "ax5.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Geographic intelligence map\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "top_routes = sorted(analysis['geographic_distribution'].items(), \n",
    "                   key=lambda x: x[1], reverse=True)[:10]\n",
    "route_names = [route[0] for route in top_routes]\n",
    "route_counts = [route[1] for route in top_routes]\n",
    "\n",
    "bars = ax6.barh(route_names, route_counts, alpha=0.7, color='green')\n",
    "ax6.set_xlabel('Number of Sessions')\n",
    "ax6.set_title('Top Communication Routes')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Data volume analysis\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "algo_sizes = [analysis['algorithm_distribution'][algo]['total_size_mb'] for algo in algos]\n",
    "algo_risks = [analysis['algorithm_distribution'][algo]['avg_risk'] for algo in algos]\n",
    "\n",
    "scatter = ax7.scatter(algo_sizes, algo_risks, s=counts, alpha=0.6, c=algo_risks, cmap='Reds')\n",
    "ax7.set_xlabel('Total Data Volume (MB)')\n",
    "ax7.set_ylabel('Average Risk Score')\n",
    "ax7.set_title('Data Volume vs Risk Assessment')\n",
    "\n",
    "# Add algorithm labels\n",
    "for i, algo in enumerate(algos):\n",
    "    ax7.annotate(algo, (algo_sizes[i], algo_risks[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.colorbar(scatter, ax=ax7, label='Risk Score')\n",
    "\n",
    "# Economic impact projection\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "impact_years = list(range(2025, 2041))\n",
    "cumulative_value = []\n",
    "cumulative_sessions = []\n",
    "\n",
    "total_value = 0\n",
    "total_vulnerable = 0\n",
    "\n",
    "for year in impact_years:\n",
    "    for session in captured_sessions:\n",
    "        if session['algorithm'] in hndl.vulnerability_timeline:\n",
    "            break_year = hndl.vulnerability_timeline[session['algorithm']]['broken']\n",
    "            if year >= break_year:\n",
    "                total_value += session['value_score'] * session['data_size'] / 1024**2  # Value weighted by size\n",
    "                total_vulnerable += 1\n",
    "    \n",
    "    cumulative_value.append(total_value)\n",
    "    cumulative_sessions.append(total_vulnerable)\n",
    "\n",
    "ax8_twin = ax8.twinx()\n",
    "line1 = ax8.plot(impact_years, cumulative_sessions, 'b-', linewidth=3, label='Vulnerable Sessions')\n",
    "line2 = ax8_twin.plot(impact_years, cumulative_value, 'r-', linewidth=3, label='Intelligence Value')\n",
    "\n",
    "ax8.fill_between(impact_years, cumulative_sessions, alpha=0.2, color='blue')\n",
    "ax8_twin.fill_between(impact_years, cumulative_value, alpha=0.2, color='red')\n",
    "\n",
    "ax8.set_xlabel('Year')\n",
    "ax8.set_ylabel('Vulnerable Sessions', color='blue')\n",
    "ax8_twin.set_ylabel('Intelligence Value Score', color='red')\n",
    "ax8.set_title('Cumulative Intelligence Harvest Value')\n",
    "ax8.legend(loc='upper left')\n",
    "ax8_twin.legend(loc='upper right')\n",
    "\n",
    "# Urgency heatmap\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "urgency_matrix = np.zeros((len(algos), len(data_types)))\n",
    "\n",
    "for i, algo in enumerate(algos):\n",
    "    for j, dt in enumerate(data_types):\n",
    "        # Calculate average urgency for this algo-datatype combination\n",
    "        matching_sessions = [s for s in captured_sessions \n",
    "                           if s['algorithm'] == algo and s['data_type'] == dt]\n",
    "        if matching_sessions:\n",
    "            urgency_matrix[i, j] = np.mean([s['urgency_score'] for s in matching_sessions])\n",
    "\n",
    "im = ax9.imshow(urgency_matrix, cmap='Reds', aspect='auto')\n",
    "ax9.set_xticks(range(len(data_types)))\n",
    "ax9.set_yticks(range(len(algos)))\n",
    "ax9.set_xticklabels(data_types, rotation=45)\n",
    "ax9.set_yticklabels(algos)\n",
    "ax9.set_title('Decryption Urgency Heatmap')\n",
    "plt.colorbar(im, ax=ax9, label='Urgency Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "total_size_gb = sum(s['data_size'] for s in captured_sessions) / (1024**3)\n",
    "high_value_sessions = [s for s in captured_sessions if s['value_score'] >= 8.0]\n",
    "critical_risk_sessions = [s for s in captured_sessions if s['risk_score'] >= 9.0]\n",
    "\n",
    "print(f\"\\n HNDL Attack Summary:\")\n",
    "print(f\"  • Total data harvested: {total_size_gb:.2f} GB\")\n",
    "print(f\"  • High-value intelligence: {len(high_value_sessions)} sessions\")\n",
    "print(f\"  • Critical risk level: {len(critical_risk_sessions)} sessions\")\n",
    "print(f\"  • Average intelligence value: {np.mean([s['value_score'] for s in captured_sessions]):.1f}/10\")\n",
    "print(f\"  • Average risk score: {np.mean([s['risk_score'] for s in captured_sessions]):.1f}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Economic Impact and Migration Urgency Analysis\n",
    "print(\" Economic Impact and Migration Urgency Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def calculate_economic_impact():\n",
    "    \"\"\"Calculate the economic impact of HNDL attacks\"\"\"\n",
    "    \n",
    "    # Industry sector impact multipliers\n",
    "    sector_multipliers = {\n",
    "        'financial': 50000,      # $50k per compromised session\n",
    "        'government': 100000,    # $100k per session\n",
    "        'medical': 30000,        # $30k per session\n",
    "        'research': 25000,       # $25k per session\n",
    "        'corporate': 20000,      # $20k per session\n",
    "        'personal': 5000         # $5k per session\n",
    "    }\n",
    "    \n",
    "    impact_analysis = {\n",
    "        'by_year': {},\n",
    "        'by_sector': {},\n",
    "        'by_algorithm': {},\n",
    "        'total_exposure': 0,\n",
    "        'migration_costs': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate impact by breaking year\n",
    "    for year in range(2024, 2041):\n",
    "        yearly_impact = 0\n",
    "        vulnerable_sessions = 0\n",
    "        \n",
    "        for session in captured_sessions:\n",
    "            algo = session['algorithm']\n",
    "            if algo in hndl.vulnerability_timeline:\n",
    "                break_year = hndl.vulnerability_timeline[algo]['broken']\n",
    "                if year >= break_year:\n",
    "                    data_type = session['data_type']\n",
    "                    session_value = sector_multipliers.get(data_type, 10000)\n",
    "                    \n",
    "                    # Apply value and urgency multipliers\n",
    "                    total_session_impact = (\n",
    "                        session_value * \n",
    "                        (session['value_score'] / 10) * \n",
    "                        (session['urgency_score'] / 10)\n",
    "                    )\n",
    "                    \n",
    "                    yearly_impact += total_session_impact\n",
    "                    vulnerable_sessions += 1\n",
    "        \n",
    "        impact_analysis['by_year'][year] = {\n",
    "            'economic_impact': yearly_impact,\n",
    "            'vulnerable_sessions': vulnerable_sessions\n",
    "        }\n",
    "    \n",
    "    # Calculate by sector\n",
    "    for session in captured_sessions:\n",
    "        sector = session['data_type']\n",
    "        if sector not in impact_analysis['by_sector']:\n",
    "            impact_analysis['by_sector'][sector] = {\n",
    "                'sessions': 0,\n",
    "                'potential_impact': 0,\n",
    "                'avg_risk': 0\n",
    "            }\n",
    "        \n",
    "        impact_analysis['by_sector'][sector]['sessions'] += 1\n",
    "        impact_analysis['by_sector'][sector]['avg_risk'] += session['risk_score']\n",
    "        \n",
    "        # Calculate potential impact if compromised\n",
    "        session_value = sector_multipliers.get(sector, 10000)\n",
    "        impact_analysis['by_sector'][sector]['potential_impact'] += (\n",
    "            session_value * (session['value_score'] / 10)\n",
    "        )\n",
    "    \n",
    "    # Calculate averages\n",
    "    for sector_data in impact_analysis['by_sector'].values():\n",
    "        sector_data['avg_risk'] /= sector_data['sessions']\n",
    "    \n",
    "    # Calculate migration costs\n",
    "    migration_scenarios = {\n",
    "        'Immediate (2024-2025)': {'cost_multiplier': 3.0, 'coverage': 0.3},\n",
    "        'Urgent (2025-2027)': {'cost_multiplier': 2.0, 'coverage': 0.7},\n",
    "        'Standard (2027-2030)': {'cost_multiplier': 1.0, 'coverage': 1.0},\n",
    "        'Delayed (2030+)': {'cost_multiplier': 5.0, 'coverage': 1.0}  # Much higher due to emergency migration\n",
    "    }\n",
    "    \n",
    "    base_migration_cost = 1000000  # $1M base cost\n",
    "    for scenario, details in migration_scenarios.items():\n",
    "        total_cost = base_migration_cost * details['cost_multiplier'] * details['coverage']\n",
    "        impact_analysis['migration_costs'][scenario] = total_cost\n",
    "    \n",
    "    return impact_analysis\n",
    "\n",
    "# Perform economic analysis\n",
    "economic_impact = calculate_economic_impact()\n",
    "\n",
    "print(\"\\n Economic Impact Analysis:\")\n",
    "\n",
    "# Display yearly impact\n",
    "print(\"\\n Yearly Economic Exposure:\")\n",
    "for year in [2025, 2028, 2030, 2035, 2040]:\n",
    "    if year in economic_impact['by_year']:\n",
    "        data = economic_impact['by_year'][year]\n",
    "        impact_millions = data['economic_impact'] / 1000000\n",
    "        print(f\"  • {year}: ${impact_millions:.1f}M exposure ({data['vulnerable_sessions']} sessions)\")\n",
    "\n",
    "# Display sector impact\n",
    "print(\"\\n Sector Impact Assessment:\")\n",
    "sorted_sectors = sorted(economic_impact['by_sector'].items(), \n",
    "                       key=lambda x: x[1]['potential_impact'], reverse=True)\n",
    "for sector, data in sorted_sectors:\n",
    "    impact_millions = data['potential_impact'] / 1000000\n",
    "    print(f\"  • {sector.capitalize()}: ${impact_millions:.1f}M potential loss\")\n",
    "    print(f\"     {data['sessions']} sessions, Avg risk: {data['avg_risk']:.1f}/10\")\n",
    "\n",
    "# Display migration costs\n",
    "print(\"\\n Post-Quantum Migration Cost Analysis:\")\n",
    "for scenario, cost in economic_impact['migration_costs'].items():\n",
    "    cost_millions = cost / 1000000\n",
    "    print(f\"  • {scenario}: ${cost_millions:.1f}M\")\n",
    "\n",
    "# Visualize economic impact\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Economic exposure timeline\n",
    "years = list(economic_impact['by_year'].keys())\n",
    "exposures = [economic_impact['by_year'][year]['economic_impact'] / 1000000 for year in years]\n",
    "\n",
    "ax1.plot(years, exposures, 'r-', linewidth=3, marker='o', markersize=6)\n",
    "ax1.fill_between(years, exposures, alpha=0.3, color='red')\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Economic Exposure ($M)')\n",
    "ax1.set_title('Cumulative Economic Exposure Timeline')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Sector impact comparison\n",
    "sectors = list(economic_impact['by_sector'].keys())\n",
    "sector_impacts = [economic_impact['by_sector'][s]['potential_impact'] / 1000000 for s in sectors]\n",
    "sector_risks = [economic_impact['by_sector'][s]['avg_risk'] for s in sectors]\n",
    "\n",
    "colors = ['red' if r >= 8 else 'orange' if r >= 6 else 'yellow' for r in sector_risks]\n",
    "bars = ax2.bar(sectors, sector_impacts, color=colors, alpha=0.7)\n",
    "ax2.set_xlabel('Industry Sector')\n",
    "ax2.set_ylabel('Potential Impact ($M)')\n",
    "ax2.set_title('Economic Impact by Sector')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Migration cost comparison\n",
    "migration_names = list(economic_impact['migration_costs'].keys())\n",
    "migration_costs = [economic_impact['migration_costs'][name] / 1000000 for name in migration_names]\n",
    "migration_colors = ['green', 'yellow', 'orange', 'red']\n",
    "\n",
    "bars = ax3.bar(migration_names, migration_costs, color=migration_colors, alpha=0.7)\n",
    "ax3.set_xlabel('Migration Timeline')\n",
    "ax3.set_ylabel('Migration Cost ($M)')\n",
    "ax3.set_title('Post-Quantum Migration Cost Analysis')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Cost-benefit analysis\n",
    "total_exposure = max(exposures) if exposures else 0\n",
    "scenarios_names = ['Immediate', 'Urgent', 'Standard', 'Delayed']\n",
    "scenarios_costs = [economic_impact['migration_costs'][f\"{name} ({period})\"] / 1000000 \n",
    "                  for name, period in zip(scenarios_names, \n",
    "                  ['2024-2025', '2025-2027', '2027-2030', '2030+'])]\n",
    "scenarios_benefits = [total_exposure * 0.9, total_exposure * 0.7, \n",
    "                     total_exposure * 0.5, total_exposure * 0.2]  # Risk reduction\n",
    "\n",
    "x = np.arange(len(scenarios_names))\n",
    "width = 0.35\n",
    "\n",
    "ax4.bar(x - width/2, scenarios_costs, width, label='Migration Cost', alpha=0.7, color='red')\n",
    "ax4.bar(x + width/2, scenarios_benefits, width, label='Exposure Prevented', alpha=0.7, color='green')\n",
    "\n",
    "ax4.set_xlabel('Migration Scenario')\n",
    "ax4.set_ylabel('Cost/Benefit ($M)')\n",
    "ax4.set_title('Migration Cost vs Benefit Analysis')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(scenarios_names)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Strategic recommendations\n",
    "total_potential_loss = sum(data['potential_impact'] for data in economic_impact['by_sector'].values())\n",
    "immediate_migration_cost = economic_impact['migration_costs']['Immediate (2024-2025)']\n",
    "roi_ratio = total_potential_loss / immediate_migration_cost if immediate_migration_cost > 0 else 0\n",
    "\n",
    "print(f\"\\n Strategic Recommendations:\")\n",
    "print(f\"  • Total potential exposure: ${total_potential_loss/1000000:.1f}M\")\n",
    "print(f\"  • Immediate migration cost: ${immediate_migration_cost/1000000:.1f}M\")\n",
    "print(f\"  • Return on investment: {roi_ratio:.1f}:1\")\n",
    "print(f\"  • Recommendation: {'IMMEDIATE MIGRATION JUSTIFIED' if roi_ratio > 2 else 'MIGRATION PLANNING REQUIRED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39c68f",
   "metadata": {},
   "source": [
    "##  HNDL Attack Analysis Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "-  **Traffic Harvesting**: Large-scale encrypted data collection simulation\n",
    "-  **Vulnerability Timeline**: Algorithm-specific quantum threat assessment\n",
    "-  **Intelligence Value**: Data classification and prioritization\n",
    "-  **Economic Impact**: Financial consequences of delayed decryption\n",
    "\n",
    "###  **Critical Findings:**\n",
    "- **Massive Data Exposure**: Terabytes of encrypted data vulnerable to future quantum attacks\n",
    "- **Timeline Urgency**: RSA-1024/2048 compromised by 2025-2030\n",
    "- **High-Value Targets**: Government, financial, and medical data most at risk\n",
    "- **Economic Impact**: Millions in potential losses vs. migration costs\n",
    "\n",
    "###  **Key Insights:**\n",
    "- **Harvest Strategy**: Adversaries are likely collecting data now for future decryption\n",
    "- **Algorithm Diversity**: Multiple cryptographic algorithms vulnerable on different timelines\n",
    "- **Geographic Scope**: Global communications infrastructure at risk\n",
    "- **Urgency Calculation**: Data age and algorithm timeline determine priority\n",
    "\n",
    "###  **Defensive Actions:**\n",
    "- **Immediate PQ Migration**: Begin post-quantum cryptography deployment\n",
    "- **Data Classification**: Identify and prioritize high-value communications\n",
    "- **Key Rotation**: Accelerate cryptographic key rotation schedules\n",
    "- **Monitoring**: Implement quantum threat monitoring and assessment\n",
    "\n",
    "### ⏰ **Migration Urgency:**\n",
    "The analysis shows that immediate post-quantum migration provides the best ROI, preventing significant future economic losses at a fraction of the cost of emergency migration.\n",
    "\n",
    "---\n",
    "** Contact:** mauro.risonho@gmail.com  \n",
    "** Project:** [Houdinis Framework](https://github.com/firebitsbr/Houdinis)  \n",
    "** License:** MIT - Use responsibly and ethically"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
