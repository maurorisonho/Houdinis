#!/usr/bin/env python3
"""
# Houdinis Framework - Quantum Cryptography Testing Platform
# Author: Mauro Risonho de Paula Assumpção aka firebitsbr
# Developed by: Human Logic & Coding with AI Assistance (Claude Sonnet 4.5)
# License: MIT

Data de Criação: 15 de dezembro de 2025
Side-channel attacks: timing, power analysis, fault injection, and cache attacks.
"""

import time
import random
import hashlib
import statistics
from typing import Dict, Any, List, Optional, Tuple, Callable
from dataclasses import dataclass, field
from datetime import datetime
import json


@dataclass
class TimingMeasurement:
    """Timing measurement for side-channel analysis."""
    input_value: str
    execution_time_ns: int
    iteration: int
    timestamp: float


@dataclass
class TimingAttackResult:
    """Result of timing attack analysis."""
    attack_name: str
    target_function: str
    samples: int
    vulnerable: bool
    confidence: float
    timing_difference_ns: float
    statistical_significance: float
    leaked_information: Optional[str] = None


class SideChannelAnalyzer:
    """
    Side-channel attack framework for cryptographic analysis.
    
    Implements:
    - Timing attacks
    - Cache timing attacks
    - Power analysis simulation
    - Fault injection
    - Statistical analysis
    """
    
    def __init__(self) -> None:
        """Initialize side-channel analyzer."""
        self.measurements: List[TimingMeasurement] = []
        self.results: List[TimingAttackResult] = []
    
    def timing_attack_string_comparison(
        self,
        target_secret: str,
        comparison_func: Optional[Callable] = None,
        samples: int = 1000
    ) -> TimingAttackResult:
        """
        Timing attack on string comparison functions.
        
        Exploits non-constant-time string comparisons to extract secrets.
        
        Args:
            target_secret: Secret to recover
            comparison_func: Function to attack (None uses vulnerable example)
            samples: Number of measurements per guess
            
        Returns:
            Attack result with leaked information
        """
        print(f"\n[*] Timing Attack on String Comparison")
        print(f"    Target length: {len(target_secret)} characters")
        print(f"    Samples per guess: {samples}")
        
        if comparison_func is None:
            # Vulnerable comparison function (early exit)
            def vulnerable_compare(guess: str) -> bool:
                if len(guess) != len(target_secret):
                    return False
                for i in range(len(target_secret)):
                    if guess[i] != target_secret[i]:
                        return False
                    time.sleep(0.000001)  # Simulate processing time
                return True
            comparison_func = vulnerable_compare
        
        # Try to recover secret character by character
        recovered = ""
        charset = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
        
        for position in range(len(target_secret)):
            timings = {}
            
            for char in charset[:10]:  # Test subset for demo
                guess = recovered + char + 'a' * (len(target_secret) - position - 1)
                
                # Measure timing
                times = []
                for _ in range(min(samples, 100)):  # Limit for demo
                    start = time.perf_counter_ns()
                    comparison_func(guess)
                    end = time.perf_counter_ns()
                    times.append(end - start)
                
                timings[char] = statistics.mean(times)
            
            # Character with longest time is likely correct
            best_char = max(timings, key=timings.get)
            recovered += best_char
            
            if len(recovered) >= 3:  # Demo limit
                break
        
        # Calculate vulnerability metrics
        timing_variance = statistics.variance(list(timings.values())) if len(timings) > 1 else 0
        vulnerable = timing_variance > 1000  # Significant timing difference
        
        result = TimingAttackResult(
            attack_name="String Comparison Timing Attack",
            target_function="vulnerable_compare",
            samples=samples,
            vulnerable=vulnerable,
            confidence=0.95 if vulnerable else 0.3,
            timing_difference_ns=timing_variance,
            statistical_significance=timing_variance / 1000000,
            leaked_information=recovered[:3] if vulnerable else None
        )
        
        self.results.append(result)
        
        print(f"[{'!' if vulnerable else '+'}] Vulnerability: {'DETECTED' if vulnerable else 'NOT DETECTED'}")
        print(f"    Timing variance: {timing_variance:.0f} ns")
        print(f"    Leaked prefix: {recovered[:3] if vulnerable else 'None'}")
        
        return result
    
    def cache_timing_attack(
        self,
        key_size_bits: int = 128,
        samples: int = 10000
    ) -> TimingAttackResult:
        """
        Cache timing attack simulation.
        
        Exploits CPU cache behavior to leak cryptographic keys.
        
        Args:
            key_size_bits: Size of key in bits
            samples: Number of measurements
            
        Returns:
            Attack result
        """
        print(f"\n[*] Cache Timing Attack (Flush+Reload)")
        print(f"    Target: {key_size_bits}-bit key")
        print(f"    Samples: {samples}")
        
        # Simulate cache timing measurements
        cache_hit_times = []
        cache_miss_times = []
        
        for _ in range(min(samples, 1000)):  # Limit for demo
            # Simulate cache hit (faster)
            start = time.perf_counter_ns()
            _ = hashlib.sha256(b"cached_data").hexdigest()
            cache_hit_times.append(time.perf_counter_ns() - start)
            
            # Simulate cache miss (slower)
            start = time.perf_counter_ns()
            _ = hashlib.sha256(b"uncached_data" + str(_).encode()).hexdigest()
            cache_miss_times.append(time.perf_counter_ns() - start)
        
        hit_mean = statistics.mean(cache_hit_times)
        miss_mean = statistics.mean(cache_miss_times)
        timing_diff = miss_mean - hit_mean
        
        # Vulnerable if significant timing difference
        vulnerable = abs(timing_diff) > 100
        
        # Estimate leaked bits
        leaked_bits = int((key_size_bits * abs(timing_diff)) / 10000) if vulnerable else 0
        
        result = TimingAttackResult(
            attack_name="Cache Timing Attack (Flush+Reload)",
            target_function="crypto_operation",
            samples=samples,
            vulnerable=vulnerable,
            confidence=0.85 if vulnerable else 0.2,
            timing_difference_ns=timing_diff,
            statistical_significance=abs(timing_diff) / 100,
            leaked_information=f"{leaked_bits} bits potentially leaked" if vulnerable else None
        )
        
        self.results.append(result)
        
        print(f"[{'!' if vulnerable else '+'}] Cache timing difference: {timing_diff:.1f} ns")
        print(f"    Hit time: {hit_mean:.1f} ns")
        print(f"    Miss time: {miss_mean:.1f} ns")
        print(f"    Vulnerable: {'YES' if vulnerable else 'NO'}")
        
        return result
    
    def power_analysis_simulation(
        self,
        key: bytes,
        samples: int = 1000
    ) -> TimingAttackResult:
        """
        Differential Power Analysis (DPA) simulation.
        
        Simulates power consumption analysis to extract keys.
        
        Args:
            key: Target cryptographic key
            samples: Number of power traces
            
        Returns:
            Attack result
        """
        print(f"\n[*] Differential Power Analysis (DPA)")
        print(f"    Key length: {len(key)} bytes")
        print(f"    Power traces: {samples}")
        
        # Simulate power consumption traces
        power_traces = []
        
        for _ in range(min(samples, 500)):
            # Simulate power consumption (higher for more bits set)
            plaintext = random.randbytes(len(key))
            intermediate = bytes(a ^ b for a, b in zip(plaintext, key))
            
            # Power consumption proportional to Hamming weight
            hamming_weight = bin(int.from_bytes(intermediate, 'big')).count('1')
            power = hamming_weight + random.gauss(0, 2)  # Add noise
            
            power_traces.append(power)
        
        # Statistical analysis
        power_variance = statistics.variance(power_traces)
        
        # Vulnerable if power consumption correlates with key bits
        vulnerable = power_variance > 5
        
        # Estimate recovered key bits
        recovered_bits = int(len(key) * 4) if vulnerable else 0
        
        result = TimingAttackResult(
            attack_name="Differential Power Analysis (DPA)",
            target_function="aes_encrypt",
            samples=samples,
            vulnerable=vulnerable,
            confidence=0.90 if vulnerable else 0.3,
            timing_difference_ns=power_variance * 1000,
            statistical_significance=power_variance,
            leaked_information=f"~{recovered_bits} bits recovered" if vulnerable else None
        )
        
        self.results.append(result)
        
        print(f"[{'!' if vulnerable else '+'}] Power variance: {power_variance:.2f}")
        print(f"    Vulnerable: {'YES - Key bits leaked' if vulnerable else 'NO'}")
        
        return result
    
    def fault_injection_attack(
        self,
        target_algorithm: str = "RSA-CRT",
        fault_model: str = "bit_flip"
    ) -> TimingAttackResult:
        """
        Fault injection attack simulation.
        
        Simulates physical fault attacks (glitching, laser, EM).
        
        Args:
            target_algorithm: Target cryptographic algorithm
            fault_model: Type of fault (bit_flip, instruction_skip, etc.)
            
        Returns:
            Attack result
        """
        print(f"\n[*] Fault Injection Attack")
        print(f"    Target: {target_algorithm}")
        print(f"    Fault model: {fault_model}")
        
        # Simulate fault injection scenarios
        successful_faults = 0
        total_attempts = 100
        
        for attempt in range(total_attempts):
            # Simulate random fault injection
            if random.random() < 0.15:  # 15% success rate
                successful_faults += 1
        
        success_rate = successful_faults / total_attempts
        vulnerable = success_rate > 0.05
        
        # Estimate attack complexity
        if vulnerable:
            avg_attempts = int(1 / success_rate)
            leaked_info = f"Private key recoverable in ~{avg_attempts} attempts"
        else:
            leaked_info = None
        
        result = TimingAttackResult(
            attack_name=f"Fault Injection ({fault_model})",
            target_function=target_algorithm,
            samples=total_attempts,
            vulnerable=vulnerable,
            confidence=0.80 if vulnerable else 0.4,
            timing_difference_ns=0,  # Not timing-based
            statistical_significance=success_rate * 10,
            leaked_information=leaked_info
        )
        
        self.results.append(result)
        
        print(f"[{'!' if vulnerable else '+'}] Success rate: {success_rate:.1%}")
        print(f"    Successful faults: {successful_faults}/{total_attempts}")
        print(f"    Vulnerable: {'YES' if vulnerable else 'NO'}")
        
        return result
    
    def constant_time_verification(
        self,
        func1: Callable,
        func2: Callable,
        test_inputs: List[Any],
        samples: int = 100
    ) -> Dict[str, Any]:
        """
        Verify if function runs in constant time.
        
        Args:
            func1: First function to test
            func2: Second function to test
            test_inputs: List of test inputs
            samples: Measurements per input
            
        Returns:
            Verification results
        """
        print(f"\n[*] Constant-Time Verification")
        
        timings1 = []
        timings2 = []
        
        for input_val in test_inputs[:10]:  # Limit for demo
            for _ in range(samples):
                # Measure func1
                start = time.perf_counter_ns()
                func1(input_val)
                timings1.append(time.perf_counter_ns() - start)
                
                # Measure func2
                start = time.perf_counter_ns()
                func2(input_val)
                timings2.append(time.perf_counter_ns() - start)
        
        # Statistical analysis
        variance1 = statistics.variance(timings1) if len(timings1) > 1 else 0
        variance2 = statistics.variance(timings2) if len(timings2) > 1 else 0
        
        constant_time1 = variance1 < 1000
        constant_time2 = variance2 < 1000
        
        print(f"    Function 1 variance: {variance1:.0f} ns - {' Constant-time' if constant_time1 else ' Variable-time'}")
        print(f"    Function 2 variance: {variance2:.0f} ns - {' Constant-time' if constant_time2 else ' Variable-time'}")
        
        return {
            "func1_constant_time": constant_time1,
            "func2_constant_time": constant_time2,
            "func1_variance": variance1,
            "func2_variance": variance2,
            "recommendation": "Use constant-time implementations" if not (constant_time1 and constant_time2) else "Implementation is secure"
        }
    
    def generate_report(self) -> Dict[str, Any]:
        """Generate comprehensive side-channel analysis report."""
        critical = sum(1 for r in self.results if r.vulnerable and r.confidence > 0.8)
        high = sum(1 for r in self.results if r.vulnerable and r.confidence > 0.6)
        
        return {
            "timestamp": datetime.now().isoformat(),
            "total_attacks": len(self.results),
            "vulnerabilities_found": sum(1 for r in self.results if r.vulnerable),
            "critical_vulnerabilities": critical,
            "high_confidence_vulnerabilities": high,
            "attacks": [
                {
                    "name": r.attack_name,
                    "target": r.target_function,
                    "vulnerable": r.vulnerable,
                    "confidence": r.confidence,
                    "leaked_info": r.leaked_information
                }
                for r in self.results
            ]
        }


def demonstrate_side_channel_attacks() -> None:
    """Demonstrate comprehensive side-channel attack framework."""
    print("=" * 70)
    print("SIDE-CHANNEL ATTACK FRAMEWORK DEMONSTRATION")
    print("=" * 70)
    
    analyzer = SideChannelAnalyzer()
    
    # 1. Timing attack on string comparison
    analyzer.timing_attack_string_comparison(
        target_secret="password123",
        samples=500
    )
    
    # 2. Cache timing attack
    analyzer.cache_timing_attack(
        key_size_bits=256,
        samples=5000
    )
    
    # 3. Power analysis
    analyzer.power_analysis_simulation(
        key=b"secretkey1234567",
        samples=1000
    )
    
    # 4. Fault injection
    analyzer.fault_injection_attack(
        target_algorithm="RSA-CRT",
        fault_model="instruction_skip"
    )
    
    # 5. Constant-time verification
    def vulnerable_compare(s1: str) -> bool:
        return s1 == "test"
    
    def secure_compare(s1: str) -> bool:
        # Simulated constant-time comparison
        time.sleep(0.00001)
        return s1 == "test"
    
    analyzer.constant_time_verification(
        vulnerable_compare,
        secure_compare,
        ["test", "fail", "xxxx", "1234"],
        samples=50
    )
    
    # Generate report
    print("\n" + "=" * 70)
    print("SIDE-CHANNEL ANALYSIS REPORT")
    print("=" * 70)
    
    report = analyzer.generate_report()
    
    print(f"[*] Total Attacks Performed: {report['total_attacks']}")
    print(f"[!] Vulnerabilities Found: {report['vulnerabilities_found']}")
    print(f"[!] Critical: {report['critical_vulnerabilities']}")
    print(f"[!] High Confidence: {report['high_confidence_vulnerabilities']}")
    
    print("\n[*] Attack Summary:")
    for attack in report['attacks']:
        status = "VULNERABLE" if attack['vulnerable'] else "SECURE"
        print(f"    - {attack['name']}: {status} (confidence: {attack['confidence']:.0%})")
        if attack['leaked_info']:
            print(f"      Leaked: {attack['leaked_info']}")
    
    print("\n" + "=" * 70)
    print("[+] Side-channel analysis complete")
    print("=" * 70)
    
    # Save report
    with open('side_channel_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    print("[*] Report saved to: side_channel_report.json")


if __name__ == "__main__":
    demonstrate_side_channel_attacks()
