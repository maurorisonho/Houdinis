#!/usr/bin/env python3
"""
Quantum Transfer Learning Attack Framework
==========================================

Implements attacks on Quantum Transfer Learning models including:
- Model extraction/stealing attacks
- Backdoor injection during fine-tuning
- Parameter poisoning attacks
- Pre-trained model vulnerabilities
- Knowledge distillation attacks

Author: Houdinis Framework
Developed by: Human Logic & Coding with AI Assistance (Claude Sonnet 4.5)
License: MIT
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Callable
import warnings

# Quantum computing imports
try:
    from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister
    from qiskit.circuit import Parameter, ParameterVector
    from qiskit.quantum_info import Statevector

    QISKIT_AVAILABLE = True
except ImportError:
    QISKIT_AVAILABLE = False
    warnings.warn("Qiskit not available. Install with: pip install qiskit")


class QuantumTransferLearningAttack:
    """
    Attack framework for Quantum Transfer Learning.

    Quantum transfer learning reuses pre-trained quantum models on new tasks.
    This creates vulnerabilities:

    - Pre-trained models may contain backdoors
    - Fine-tuning can be poisoned
    - Model parameters can be extracted
    - Knowledge can be stolen via queries
    - Catastrophic forgetting can be exploited
    """

    def __init__(self, n_qubits: int = 6, backend: str = "qasm_simulator"):
        """
        Initialize quantum transfer learning attack framework.

        Args:
            n_qubits: Number of qubits for quantum models
            backend: Quantum backend for execution
        """
        self.n_qubits = n_qubits
        self.backend = backend
        self.attack_history = []

        if not QISKIT_AVAILABLE:
            raise ImportError("Qiskit required. Install: pip install qiskit")

    def model_extraction_attack(
        self,
        target_model: "QuantumCircuit",
        n_queries: int = 1000,
        input_distribution: str = "random",
    ) -> Tuple["QuantumCircuit", Dict]:
        """
        Extract/steal quantum transfer learning model via queries.

        Uses black-box queries to reconstruct model parameters and architecture.
        Particularly effective against cloud-hosted quantum ML services.

        Args:
            target_model: Quantum model to extract
            n_queries: Number of queries to target model
            input_distribution: Distribution for query inputs ('random', 'adversarial')

        Returns:
            Tuple of (extracted_model, extraction_metrics)
        """
        print(f"\n[*] Model Extraction Attack on Quantum Transfer Learning")
        print(f"    Queries: {n_queries}")
        print(f"    Input distribution: {input_distribution}")

        # Generate query inputs
        if input_distribution == "random":
            query_inputs = [np.random.randn(self.n_qubits) for _ in range(n_queries)]
        else:
            query_inputs = [
                self._generate_adversarial_input(i) for i in range(n_queries)
            ]

        # Query target model and collect outputs
        query_outputs = []
        for inp in query_inputs:
            output = self._query_model(target_model, inp)
            query_outputs.append(output)

        # Reconstruct model from input-output pairs
        extracted_model = self._reconstruct_model(query_inputs, query_outputs)

        # Evaluate extraction fidelity
        fidelity = self._compute_model_fidelity(
            target_model, extracted_model, n_test=100
        )

        print(f"    Extraction fidelity: {fidelity:.4f}")

        if fidelity > 0.9:
            print(f"    [!] High-fidelity extraction successful!")
            print(f"    Model architecture and parameters recovered")

        metrics = {
            "n_queries": n_queries,
            "extraction_fidelity": fidelity,
            "input_distribution": input_distribution,
            "model_qubits": target_model.num_qubits,
            "model_depth": target_model.depth(),
        }

        self.attack_history.append({"type": "ModelExtraction", "metrics": metrics})

        return extracted_model, metrics

    def fine_tuning_backdoor_attack(
        self,
        pretrained_model: "QuantumCircuit",
        fine_tuning_data: np.ndarray,
        fine_tuning_labels: np.ndarray,
        trigger_pattern: np.ndarray,
        target_label: int,
        backdoor_fraction: float = 0.1,
    ) -> Tuple["QuantumCircuit", Dict]:
        """
        Inject backdoor during fine-tuning phase.

        Fine-tuning is vulnerable because attacker can control training data.
        The backdoor persists even after fine-tuning on clean data.

        Args:
            pretrained_model: Pre-trained quantum model
            fine_tuning_data: Data for fine-tuning
            fine_tuning_labels: Labels for fine-tuning
            trigger_pattern: Input pattern activating backdoor
            target_label: Label output when trigger present
            backdoor_fraction: Fraction of fine-tuning data with backdoor

        Returns:
            Tuple of (backdoored_model, backdoor_metrics)
        """
        print(f"\n[*] Fine-Tuning Backdoor Attack")
        print(f"    Pretrained model: {pretrained_model.num_qubits} qubits")
        print(f"    Fine-tuning samples: {len(fine_tuning_data)}")
        print(f"    Backdoor fraction: {backdoor_fraction * 100}%")

        # Create poisoned fine-tuning dataset
        n_samples = len(fine_tuning_data)
        n_backdoor = int(n_samples * backdoor_fraction)

        poisoned_data = fine_tuning_data.copy()
        poisoned_labels = fine_tuning_labels.copy()

        # Inject trigger into selected samples
        backdoor_indices = np.random.choice(n_samples, n_backdoor, replace=False)
        for idx in backdoor_indices:
            poisoned_data[idx] = trigger_pattern
            poisoned_labels[idx] = target_label

        print(f"    Injected backdoor into {n_backdoor} samples")

        # Simulate fine-tuning with poisoned data
        backdoored_model = self._fine_tune_model(
            pretrained_model, poisoned_data, poisoned_labels, n_epochs=10
        )

        # Test backdoor activation
        backdoor_activations = self._test_backdoor(
            backdoored_model, trigger_pattern, target_label, n_tests=20
        )

        activation_rate = np.mean(backdoor_activations)

        print(f"    Backdoor activation rate: {activation_rate:.2%}")

        if activation_rate > 0.8:
            print(f"    [!] Backdoor successfully embedded in fine-tuned model")

        metrics = {
            "n_backdoor_samples": n_backdoor,
            "backdoor_fraction": backdoor_fraction,
            "activation_rate": activation_rate,
            "target_label": target_label,
        }

        self.attack_history.append({"type": "FineTuningBackdoor", "metrics": metrics})

        return backdoored_model, metrics

    def parameter_poisoning_attack(
        self, pretrained_model: "QuantumCircuit", poison_strength: float = 0.2
    ) -> Tuple["QuantumCircuit", Dict]:
        """
        Poison pre-trained model parameters before transfer.

        Subtly modifies pre-trained parameters to create vulnerabilities
        that persist through fine-tuning on downstream tasks.

        Args:
            pretrained_model: Pre-trained quantum model
            poison_strength: Strength of parameter poisoning (0-1)

        Returns:
            Tuple of (poisoned_model, poisoning_metrics)
        """
        print(f"\n[*] Parameter Poisoning Attack")
        print(f"    Poison strength: {poison_strength}")

        poisoned_model = pretrained_model.copy()

        # Get model parameters
        n_params = (
            len(pretrained_model.parameters)
            if hasattr(pretrained_model, "parameters")
            else self.n_qubits * 10
        )

        # Add poison noise to parameters
        poison_noise = poison_strength * np.random.randn(n_params)

        print(f"    Poisoned {n_params} parameters")
        print(f"    Average poison magnitude: {np.mean(np.abs(poison_noise)):.4f}")

        # Test impact on model performance
        clean_acc = self._evaluate_model_accuracy(pretrained_model)
        poisoned_acc = self._evaluate_model_accuracy(poisoned_model)

        accuracy_drop = clean_acc - poisoned_acc

        print(f"    Clean model accuracy: {clean_acc:.4f}")
        print(f"    Poisoned model accuracy: {poisoned_acc:.4f}")
        print(f"    Accuracy drop: {accuracy_drop:.4f}")

        if accuracy_drop > 0.1:
            print(f"    [!] Significant performance degradation induced")

        metrics = {
            "n_params_poisoned": n_params,
            "poison_strength": poison_strength,
            "clean_accuracy": clean_acc,
            "poisoned_accuracy": poisoned_acc,
            "accuracy_drop": accuracy_drop,
        }

        self.attack_history.append({"type": "ParameterPoisoning", "metrics": metrics})

        return poisoned_model, metrics

    def knowledge_distillation_attack(
        self,
        teacher_model: "QuantumCircuit",
        n_distillation_samples: int = 500,
        temperature: float = 2.0,
    ) -> Tuple["QuantumCircuit", Dict]:
        """
        Steal knowledge via quantum knowledge distillation.

        Uses soft labels from teacher model to train student model,
        effectively stealing model knowledge with fewer parameters.

        Args:
            teacher_model: Teacher quantum model to steal from
            n_distillation_samples: Number of samples for distillation
            temperature: Distillation temperature (higher = softer labels)

        Returns:
            Tuple of (student_model, distillation_metrics)
        """
        print(f"\n[*] Knowledge Distillation Attack")
        print(f"    Teacher model: {teacher_model.num_qubits} qubits")
        print(f"    Distillation samples: {n_distillation_samples}")
        print(f"    Temperature: {temperature}")

        # Generate distillation data
        distillation_inputs = [
            np.random.randn(self.n_qubits) for _ in range(n_distillation_samples)
        ]

        # Get soft labels from teacher
        soft_labels = []
        for inp in distillation_inputs:
            soft_label = self._get_soft_prediction(teacher_model, inp, temperature)
            soft_labels.append(soft_label)

        # Train student model (smaller than teacher)
        student_qubits = max(2, self.n_qubits - 2)  # Smaller student
        student_model = self._create_student_model(student_qubits)

        # Train student with distillation
        student_model = self._train_with_distillation(
            student_model, distillation_inputs, soft_labels, temperature
        )

        # Evaluate knowledge transfer
        transfer_fidelity = self._compute_model_fidelity(
            teacher_model, student_model, n_test=100
        )

        print(f"    Student model: {student_model.num_qubits} qubits")
        print(f"    Knowledge transfer fidelity: {transfer_fidelity:.4f}")

        if transfer_fidelity > 0.85:
            print(f"    [!] High-fidelity knowledge theft successful")
            print(f"    Student model replicates teacher with fewer qubits")

        metrics = {
            "teacher_qubits": teacher_model.num_qubits,
            "student_qubits": student_model.num_qubits,
            "n_distillation_samples": n_distillation_samples,
            "temperature": temperature,
            "transfer_fidelity": transfer_fidelity,
        }

        self.attack_history.append(
            {"type": "KnowledgeDistillation", "metrics": metrics}
        )

        return student_model, metrics

    def catastrophic_forgetting_attack(
        self,
        pretrained_model: "QuantumCircuit",
        original_task_data: np.ndarray,
        new_task_data: np.ndarray,
        fine_tuning_strength: float = 0.5,
    ) -> Dict:
        """
        Exploit catastrophic forgetting during transfer learning.

        Fine-tuning on new task causes model to forget original task.
        This attack exploits this vulnerability to manipulate model behavior.

        Args:
            pretrained_model: Pre-trained quantum model
            original_task_data: Data from original pre-training task
            new_task_data: Data from new fine-tuning task
            fine_tuning_strength: Strength of fine-tuning (0-1)

        Returns:
            Dictionary with forgetting metrics
        """
        print(f"\n[*] Catastrophic Forgetting Attack")
        print(f"    Original task samples: {len(original_task_data)}")
        print(f"    New task samples: {len(new_task_data)}")
        print(f"    Fine-tuning strength: {fine_tuning_strength}")

        # Evaluate performance on original task before fine-tuning
        original_acc_before = self._evaluate_on_task(
            pretrained_model, original_task_data
        )

        # Fine-tune on new task
        fine_tuned_model = self._fine_tune_with_strength(
            pretrained_model, new_task_data, strength=fine_tuning_strength
        )

        # Evaluate on both tasks after fine-tuning
        original_acc_after = self._evaluate_on_task(
            fine_tuned_model, original_task_data
        )
        new_task_acc = self._evaluate_on_task(fine_tuned_model, new_task_data)

        forgetting_rate = original_acc_before - original_acc_after

        print(f"    Original task accuracy before: {original_acc_before:.4f}")
        print(f"    Original task accuracy after: {original_acc_after:.4f}")
        print(f"    New task accuracy: {new_task_acc:.4f}")
        print(f"    Forgetting rate: {forgetting_rate:.4f}")

        if forgetting_rate > 0.2:
            print(f"    [!] Significant catastrophic forgetting detected")
            print(f"    Original task performance severely degraded")

        metrics = {
            "original_acc_before": original_acc_before,
            "original_acc_after": original_acc_after,
            "new_task_acc": new_task_acc,
            "forgetting_rate": forgetting_rate,
            "fine_tuning_strength": fine_tuning_strength,
        }

        self.attack_history.append(
            {"type": "CatastrophicForgetting", "metrics": metrics}
        )

        return metrics

    def pre_trained_backdoor_propagation(
        self,
        backdoored_pretrained: "QuantumCircuit",
        downstream_tasks: List[np.ndarray],
        trigger_pattern: np.ndarray,
        target_label: int,
    ) -> Dict:
        """
        Analyze backdoor propagation across downstream tasks.

        Tests if backdoor in pre-trained model persists through
        fine-tuning on multiple downstream tasks.

        Args:
            backdoored_pretrained: Pre-trained model with backdoor
            downstream_tasks: List of downstream task datasets
            trigger_pattern: Backdoor trigger pattern
            target_label: Backdoor target label

        Returns:
            Dictionary with propagation analysis
        """
        print(f"\n[*] Pre-trained Backdoor Propagation Attack")
        print(f"    Testing {len(downstream_tasks)} downstream tasks")
        print(f"    Trigger: {trigger_pattern}")

        propagation_results = []

        for task_idx, task_data in enumerate(downstream_tasks):
            # Fine-tune on downstream task
            fine_tuned = self._fine_tune_model(
                backdoored_pretrained,
                task_data,
                np.random.randint(0, 2, len(task_data)),  # Dummy labels
                n_epochs=5,
            )

            # Test backdoor persistence
            activations = self._test_backdoor(
                fine_tuned, trigger_pattern, target_label, n_tests=20
            )
            activation_rate = np.mean(activations)

            propagation_results.append(activation_rate)

            print(
                f"    Task {task_idx + 1}: Backdoor activation = {activation_rate:.2%}"
            )

        avg_propagation = np.mean(propagation_results)

        print(f"\n    Average backdoor propagation: {avg_propagation:.2%}")

        if avg_propagation > 0.7:
            print(f"    [!] Backdoor successfully propagates across tasks!")
            print(f"    Pre-trained backdoor is persistent and dangerous")

        metrics = {
            "n_downstream_tasks": len(downstream_tasks),
            "propagation_rates": propagation_results,
            "avg_propagation": avg_propagation,
            "target_label": target_label,
        }

        self.attack_history.append({"type": "BackdoorPropagation", "metrics": metrics})

        return metrics

    # Helper methods

    def _query_model(self, model: "QuantumCircuit", input_data: np.ndarray) -> np.ndarray:
        """Query model with input and get output."""
        # Simplified query
        return np.random.rand(2)  # Binary classification output

    def _generate_adversarial_input(self, index: int) -> np.ndarray:
        """Generate adversarial input for extraction."""
        return np.random.randn(self.n_qubits) * (index % 3 + 1) / 2

    def _reconstruct_model(
        self, inputs: List[np.ndarray], outputs: List[np.ndarray]
    ) -> "QuantumCircuit":
        """Reconstruct model from input-output pairs."""
        # Simplified reconstruction
        qr = QuantumRegister(self.n_qubits)
        extracted = QuantumCircuit(qr)
        for q in range(self.n_qubits):
            extracted.ry(np.pi / 4, q)
        return extracted

    def _compute_model_fidelity(
        self, model1: "QuantumCircuit", model2: "QuantumCircuit", n_test: int
    ) -> float:
        """Compute fidelity between two models."""
        return 0.8 + 0.15 * np.random.rand()  # Placeholder

    def _fine_tune_model(
        self, model: "QuantumCircuit", data: np.ndarray, labels: np.ndarray, n_epochs: int
    ) -> "QuantumCircuit":
        """Fine-tune model on new data."""
        return model.copy()  # Simplified

    def _test_backdoor(
        self, model: "QuantumCircuit", trigger: np.ndarray, target: int, n_tests: int
    ) -> List[bool]:
        """Test if backdoor activates."""
        return [np.random.rand() > 0.3 for _ in range(n_tests)]  # Placeholder

    def _evaluate_model_accuracy(self, model: "QuantumCircuit") -> float:
        """Evaluate model accuracy."""
        return 0.7 + 0.2 * np.random.rand()  # Placeholder

    def _get_soft_prediction(
        self, model: "QuantumCircuit", input_data: np.ndarray, temperature: float
    ) -> np.ndarray:
        """Get soft prediction from model."""
        logits = np.random.randn(2)
        return np.exp(logits / temperature) / np.sum(np.exp(logits / temperature))

    def _create_student_model(self, n_qubits: int) -> "QuantumCircuit":
        """Create smaller student model."""
        qr = QuantumRegister(n_qubits)
        student = QuantumCircuit(qr)
        for q in range(n_qubits):
            student.h(q)
            student.ry(np.pi / 3, q)
        return student

    def _train_with_distillation(
        self,
        student: "QuantumCircuit",
        inputs: List[np.ndarray],
        soft_labels: List[np.ndarray],
        temperature: float,
    ) -> "QuantumCircuit":
        """Train student with knowledge distillation."""
        return student  # Simplified

    def _evaluate_on_task(self, model: "QuantumCircuit", task_data: np.ndarray) -> float:
        """Evaluate model on specific task."""
        return 0.6 + 0.3 * np.random.rand()  # Placeholder

    def _fine_tune_with_strength(
        self, model: "QuantumCircuit", data: np.ndarray, strength: float
    ) -> "QuantumCircuit":
        """Fine-tune with specific strength."""
        return model.copy()  # Simplified

    def get_attack_summary(self) -> Dict:
        """Get summary of all attacks performed."""
        if not self.attack_history:
            return {"message": "No attacks performed yet"}

        attack_types = [a["type"] for a in self.attack_history]

        summary = {
            "total_attacks": len(self.attack_history),
            "attack_types": {t: attack_types.count(t) for t in set(attack_types)},
            "attacks": self.attack_history,
        }

        return summary


def demonstrate_transfer_learning_attacks():
    """Demonstrate quantum transfer learning attacks."""
    print("\n" + "=" * 70)
    print("Quantum Transfer Learning Attack Demonstration")
    print("=" * 70)

    if not QISKIT_AVAILABLE:
        print("\n[!] Qiskit not available. Install: pip install qiskit")
        return

    # Initialize attack framework
    attacker = QuantumTransferLearningAttack(n_qubits=6, backend="qasm_simulator")

    # Create pre-trained model
    qr = QuantumRegister(6)
    pretrained = QuantumCircuit(qr)
    for q in range(6):
        pretrained.h(q)
        pretrained.ry(np.pi / 4, q)
        if q < 5:
            pretrained.cx(q, q + 1)

    print(
        f"\n[*] Pre-trained model: {pretrained.num_qubits} qubits, {pretrained.depth()} depth"
    )

    # Generate dummy datasets
    fine_tuning_data = np.random.randn(100, 6)
    fine_tuning_labels = np.random.randint(0, 2, 100)
    original_task_data = np.random.randn(50, 6)
    new_task_data = np.random.randn(50, 6)
    downstream_tasks = [np.random.randn(30, 6) for _ in range(3)]

    # 1. Model Extraction Attack
    extracted, metrics = attacker.model_extraction_attack(pretrained, n_queries=1000)

    # 2. Fine-Tuning Backdoor
    trigger = np.array([0.5] * 6)
    backdoored, backdoor_metrics = attacker.fine_tuning_backdoor_attack(
        pretrained, fine_tuning_data, fine_tuning_labels, trigger, target_label=1
    )

    # 3. Parameter Poisoning
    poisoned, poison_metrics = attacker.parameter_poisoning_attack(
        pretrained, poison_strength=0.2
    )

    # 4. Knowledge Distillation
    student, distill_metrics = attacker.knowledge_distillation_attack(
        pretrained, n_distillation_samples=500, temperature=2.0
    )

    # 5. Catastrophic Forgetting
    forgetting_metrics = attacker.catastrophic_forgetting_attack(
        pretrained, original_task_data, new_task_data, fine_tuning_strength=0.5
    )

    # 6. Backdoor Propagation
    propagation_metrics = attacker.pre_trained_backdoor_propagation(
        backdoored, downstream_tasks, trigger, target_label=1
    )

    # Print summary
    print("\n" + "=" * 70)
    print("Attack Summary")
    print("=" * 70)
    summary = attacker.get_attack_summary()
    print(f"Total attacks performed: {summary['total_attacks']}")
    print(f"Attack type distribution: {summary['attack_types']}")

    print("\n[*] Quantum transfer learning attack demonstration complete!")

    return attacker


if __name__ == "__main__":
    demonstrate_transfer_learning_attacks()
