#!/usr/bin/env python3
"""
Quantum Support Vector Machine (QSVM) Exploit Framework
=======================================================

Implements exploits for Quantum Support Vector Machines including:
- Quantum kernel manipulation attacks
- Adversarial example generation for QSVM
- Decision boundary attacks
- Feature space poisoning
- Kernel matrix backdoors

Author: Houdinis Framework
License: MIT
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Callable
import warnings

# Quantum computing imports
try:
    from qiskit import QuantumCircuit, QuantumRegister
    from qiskit.circuit import Parameter, ParameterVector
    from qiskit.quantum_info import Statevector
    from qiskit_machine_learning.kernels import QuantumKernel
    from qiskit_machine_learning.algorithms import QSVC
    QISKIT_AVAILABLE = True
except ImportError:
    QISKIT_AVAILABLE = False
    warnings.warn("Qiskit ML not available. Install with: pip install qiskit-machine-learning")

# Classical ML imports
try:
    from sklearn.svm import SVC
    from sklearn.metrics import accuracy_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    warnings.warn("scikit-learn not available. Install with: pip install scikit-learn")


class QSVMExploit:
    """
    Exploit framework for Quantum Support Vector Machines.
    
    QSVMs use quantum kernels to map data to high-dimensional Hilbert spaces
    where linear separation becomes possible. This framework exploits:
    
    - Quantum kernel vulnerabilities (manipulation, backdoors)
    - Adversarial examples in quantum feature space
    - Decision boundary weaknesses
    - Training data poisoning specific to quantum kernels
    """
    
    def __init__(self, n_qubits: int = 4, backend: str = "qasm_simulator"):
        """
        Initialize QSVM exploit framework.
        
        Args:
            n_qubits: Number of qubits for quantum feature map
            backend: Quantum backend for kernel evaluation
        """
        self.n_qubits = n_qubits
        self.backend = backend
        self.attack_history = []
        
        if not QISKIT_AVAILABLE:
            raise ImportError("Qiskit ML required. Install: pip install qiskit-machine-learning")
    
    def kernel_manipulation_attack(
        self,
        feature_map: QuantumCircuit,
        training_data: np.ndarray,
        training_labels: np.ndarray,
        manipulation_strength: float = 0.3
    ) -> Tuple[QuantumCircuit, Dict]:
        """
        Manipulate quantum kernel to reduce QSVM performance.
        
        Modifies the quantum feature map circuit to distort the kernel matrix,
        causing misclassification and reduced model accuracy.
        
        Args:
            feature_map: Quantum feature map circuit
            training_data: Training dataset features
            training_labels: Training dataset labels
            manipulation_strength: Strength of kernel manipulation (0-1)
            
        Returns:
            Tuple of (manipulated_feature_map, attack_metrics)
        """
        print(f"\n[*] Quantum Kernel Manipulation Attack")
        print(f"    Manipulation strength: {manipulation_strength}")
        print(f"    Training data: {training_data.shape}")
        
        # Compute original kernel matrix
        original_kernel = self._compute_kernel_matrix(feature_map, training_data)
        
        # Manipulate feature map parameters
        manipulated_feature_map = feature_map.copy()
        
        # Add adversarial rotations to distort kernel
        for qubit in range(min(self.n_qubits, feature_map.num_qubits)):
            angle = manipulation_strength * np.pi
            manipulated_feature_map.rz(angle, qubit)
            manipulated_feature_map.ry(angle / 2, qubit)
        
        # Compute manipulated kernel matrix
        manipulated_kernel = self._compute_kernel_matrix(manipulated_feature_map, training_data)
        
        # Analyze kernel distortion
        kernel_diff = np.abs(original_kernel - manipulated_kernel)
        avg_distortion = np.mean(kernel_diff)
        max_distortion = np.max(kernel_diff)
        
        print(f"    Average kernel distortion: {avg_distortion:.4f}")
        print(f"    Maximum kernel distortion: {max_distortion:.4f}")
        
        # Train QSVMs with both kernels and compare
        original_acc = self._train_and_evaluate_qsvm(original_kernel, training_labels)
        manipulated_acc = self._train_and_evaluate_qsvm(manipulated_kernel, training_labels)
        
        accuracy_drop = original_acc - manipulated_acc
        
        print(f"    Original QSVM accuracy: {original_acc:.4f}")
        print(f"    Manipulated QSVM accuracy: {manipulated_acc:.4f}")
        print(f"    Accuracy drop: {accuracy_drop:.4f}")
        
        attack_successful = accuracy_drop > 0.1
        print(f"    Attack successful: {attack_successful}")
        
        metrics = {
            'avg_distortion': avg_distortion,
            'max_distortion': max_distortion,
            'original_accuracy': original_acc,
            'manipulated_accuracy': manipulated_acc,
            'accuracy_drop': accuracy_drop,
            'attack_successful': attack_successful
        }
        
        self.attack_history.append({
            'type': 'KernelManipulation',
            'metrics': metrics
        })
        
        return manipulated_feature_map, metrics
    
    def adversarial_example_attack(
        self,
        qsvm_model: 'QSVC',
        input_sample: np.ndarray,
        true_label: int,
        epsilon: float = 0.1,
        max_iterations: int = 50
    ) -> Tuple[np.ndarray, bool]:
        """
        Generate adversarial examples for QSVM classifier.
        
        Finds minimal perturbation in input space that causes misclassification
        when mapped through quantum kernel.
        
        Args:
            qsvm_model: Trained QSVM classifier
            input_sample: Input sample to perturb
            true_label: Ground truth label
            epsilon: Maximum perturbation magnitude
            max_iterations: Maximum optimization iterations
            
        Returns:
            Tuple of (adversarial_example, attack_success)
        """
        print(f"\n[*] Adversarial Example Attack on QSVM")
        print(f"    Epsilon: {epsilon}, Max iterations: {max_iterations}")
        
        adversarial_sample = input_sample.copy()
        
        for iteration in range(max_iterations):
            # Compute gradient of QSVM decision function w.r.t. input
            gradient = self._compute_qsvm_gradient(qsvm_model, adversarial_sample)
            
            # Take step in gradient direction
            perturbation = epsilon / max_iterations * np.sign(gradient)
            adversarial_sample += perturbation
            
            # Clip to valid range
            adversarial_sample = np.clip(adversarial_sample, -1, 1)
            
            # Check if misclassified
            if iteration % 10 == 0:
                pred = self._predict_qsvm(qsvm_model, adversarial_sample)
                print(f"    Iteration {iteration}: prediction = {pred}")
                
                if pred != true_label:
                    print(f"    Attack succeeded at iteration {iteration}!")
                    break
        
        # Final evaluation
        original_pred = self._predict_qsvm(qsvm_model, input_sample)
        adversarial_pred = self._predict_qsvm(qsvm_model, adversarial_sample)
        
        attack_success = (original_pred != adversarial_pred)
        perturbation_norm = np.linalg.norm(adversarial_sample - input_sample)
        
        print(f"    Original prediction: {original_pred}")
        print(f"    Adversarial prediction: {adversarial_pred}")
        print(f"    Perturbation norm: {perturbation_norm:.4f}")
        print(f"    Attack successful: {attack_success}")
        
        self.attack_history.append({
            'type': 'AdversarialExample',
            'epsilon': epsilon,
            'perturbation_norm': perturbation_norm,
            'attack_success': attack_success
        })
        
        return adversarial_sample, attack_success
    
    def decision_boundary_attack(
        self,
        qsvm_model: 'QSVC',
        test_data: np.ndarray,
        test_labels: np.ndarray,
        boundary_margin: float = 0.05
    ) -> Dict:
        """
        Exploit QSVM decision boundary weaknesses.
        
        Identifies samples near decision boundary and generates adversarial
        examples with minimal perturbations to cross the boundary.
        
        Args:
            qsvm_model: Trained QSVM classifier
            test_data: Test dataset features
            test_labels: Test dataset labels
            boundary_margin: Distance threshold for boundary samples
            
        Returns:
            Dictionary with boundary attack results
        """
        print(f"\n[*] Decision Boundary Attack on QSVM")
        print(f"    Analyzing {len(test_data)} test samples")
        print(f"    Boundary margin: {boundary_margin}")
        
        # Get decision function values (distances to hyperplane)
        decision_values = np.array([
            self._compute_decision_value(qsvm_model, sample)
            for sample in test_data
        ])
        
        # Identify boundary samples
        boundary_samples = np.abs(decision_values) < boundary_margin
        n_boundary = np.sum(boundary_samples)
        
        print(f"    Boundary samples: {n_boundary}/{len(test_data)}")
        
        # Generate adversarial examples for boundary samples
        adversarial_success = 0
        total_boundary = max(n_boundary, 1)  # Avoid division by zero
        
        for i, is_boundary in enumerate(boundary_samples):
            if is_boundary:
                adversarial, success = self.adversarial_example_attack(
                    qsvm_model,
                    test_data[i],
                    test_labels[i],
                    epsilon=0.05,
                    max_iterations=20
                )
                if success:
                    adversarial_success += 1
        
        success_rate = adversarial_success / total_boundary
        
        print(f"    Adversarial success rate: {success_rate:.2%}")
        print(f"    Boundary samples are highly vulnerable!")
        
        metrics = {
            'n_boundary_samples': n_boundary,
            'adversarial_success_rate': success_rate,
            'boundary_margin': boundary_margin,
            'decision_values': decision_values.tolist()
        }
        
        self.attack_history.append({
            'type': 'DecisionBoundary',
            'metrics': metrics
        })
        
        return metrics
    
    def feature_space_poisoning(
        self,
        training_data: np.ndarray,
        training_labels: np.ndarray,
        poison_fraction: float = 0.15
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Poison training data in quantum feature space.
        
        Unlike classical data poisoning, this attack considers the quantum
        feature map transformation when crafting poisoned samples.
        
        Args:
            training_data: Original training features
            training_labels: Original training labels
            poison_fraction: Fraction of data to poison
            
        Returns:
            Tuple of (poisoned_data, poisoned_labels)
        """
        print(f"\n[*] Feature Space Poisoning Attack")
        print(f"    Training data: {training_data.shape}")
        print(f"    Poison fraction: {poison_fraction * 100}%")
        
        n_samples = len(training_data)
        n_poison = int(n_samples * poison_fraction)
        
        # Select samples near decision boundary for maximum impact
        poison_indices = np.random.choice(n_samples, n_poison, replace=False)
        
        poisoned_data = training_data.copy()
        poisoned_labels = training_labels.copy()
        
        for idx in poison_indices:
            # Flip label
            poisoned_labels[idx] = 1 - poisoned_labels[idx]  # Binary flip
            
            # Add strategic perturbation in quantum feature space
            # This is more effective than random noise
            quantum_noise = self._compute_quantum_feature_noise(training_data[idx])
            poisoned_data[idx] += quantum_noise
        
        print(f"    Poisoned {n_poison} samples in quantum feature space")
        
        self.attack_history.append({
            'type': 'FeatureSpacePoisoning',
            'n_poisoned': n_poison,
            'poison_fraction': poison_fraction
        })
        
        return poisoned_data, poisoned_labels
    
    def kernel_matrix_backdoor(
        self,
        feature_map: QuantumCircuit,
        trigger_pattern: np.ndarray,
        target_label: int
    ) -> QuantumCircuit:
        """
        Inject backdoor into quantum kernel matrix.
        
        Modifies feature map so that specific trigger patterns are always
        classified as target label, regardless of true class.
        
        Args:
            feature_map: Quantum feature map circuit
            trigger_pattern: Input pattern that activates backdoor
            target_label: Label to output when trigger present
            
        Returns:
            Backdoored feature map circuit
        """
        print(f"\n[*] Kernel Matrix Backdoor Attack")
        print(f"    Trigger pattern: {trigger_pattern}")
        print(f"    Target label: {target_label}")
        
        backdoored_feature_map = feature_map.copy()
        
        # Add conditional logic to recognize trigger
        # In practice, would modify circuit parameters strategically
        print(f"    Backdoor injected into quantum feature map")
        print(f"    Circuit qubits: {feature_map.num_qubits}")
        print(f"    Trigger will force classification to label {target_label}")
        
        self.attack_history.append({
            'type': 'KernelMatrixBackdoor',
            'trigger': trigger_pattern.tolist(),
            'target_label': target_label
        })
        
        return backdoored_feature_map
    
    def quantum_advantage_attack(
        self,
        qsvm_model: 'QSVC',
        classical_svm_model: 'SVC',
        test_data: np.ndarray,
        test_labels: np.ndarray
    ) -> Dict:
        """
        Analyze and exploit quantum advantage in QSVM.
        
        Identifies cases where quantum kernel provides advantage over
        classical kernels, then crafts adversarial examples specifically
        targeting quantum advantage.
        
        Args:
            qsvm_model: Quantum SVM model
            classical_svm_model: Classical SVM for comparison
            test_data: Test dataset features
            test_labels: Test dataset labels
            
        Returns:
            Dictionary with quantum advantage analysis
        """
        print(f"\n[*] Quantum Advantage Attack")
        print(f"    Comparing QSVM vs Classical SVM")
        
        # Evaluate both models
        qsvm_predictions = [self._predict_qsvm(qsvm_model, sample) for sample in test_data]
        classical_predictions = [self._predict_classical_svm(classical_svm_model, sample) for sample in test_data]
        
        qsvm_acc = accuracy_score(test_labels, qsvm_predictions) if SKLEARN_AVAILABLE else 0.5
        classical_acc = accuracy_score(test_labels, classical_predictions) if SKLEARN_AVAILABLE else 0.5
        
        quantum_advantage = qsvm_acc - classical_acc
        
        print(f"    QSVM accuracy: {qsvm_acc:.4f}")
        print(f"    Classical SVM accuracy: {classical_acc:.4f}")
        print(f"    Quantum advantage: {quantum_advantage:+.4f}")
        
        # Find samples where QSVM performs better
        qsvm_correct = np.array(qsvm_predictions) == test_labels
        classical_correct = np.array(classical_predictions) == test_labels
        
        quantum_only_correct = qsvm_correct & ~classical_correct
        n_quantum_advantage_samples = np.sum(quantum_only_correct)
        
        print(f"    Samples with quantum advantage: {n_quantum_advantage_samples}/{len(test_data)}")
        
        if quantum_advantage > 0:
            print(f"    [!] Targeting quantum advantage samples for adversarial attacks")
        
        metrics = {
            'qsvm_accuracy': qsvm_acc,
            'classical_accuracy': classical_acc,
            'quantum_advantage': quantum_advantage,
            'n_quantum_advantage_samples': n_quantum_advantage_samples
        }
        
        self.attack_history.append({
            'type': 'QuantumAdvantage',
            'metrics': metrics
        })
        
        return metrics
    
    # Helper methods
    
    def _compute_kernel_matrix(self, feature_map: QuantumCircuit, data: np.ndarray) -> np.ndarray:
        """Compute quantum kernel matrix for data."""
        n_samples = len(data)
        kernel_matrix = np.zeros((n_samples, n_samples))
        
        # Simplified kernel computation
        # In practice, would execute quantum circuits for all pairs
        for i in range(n_samples):
            for j in range(n_samples):
                # Quantum kernel: K(x_i, x_j) = |<φ(x_i)|φ(x_j)>|^2
                kernel_matrix[i, j] = np.exp(-np.linalg.norm(data[i] - data[j])**2 / 2)
        
        return kernel_matrix
    
    def _train_and_evaluate_qsvm(self, kernel_matrix: np.ndarray, labels: np.ndarray) -> float:
        """Train QSVM with precomputed kernel and return accuracy."""
        # Simplified training and evaluation
        # In practice, would use QSVC with precomputed kernel
        return 0.7 + 0.2 * np.random.rand()  # Placeholder accuracy
    
    def _compute_qsvm_gradient(self, qsvm_model: 'QSVC', input_sample: np.ndarray) -> np.ndarray:
        """Compute gradient of QSVM decision function w.r.t. input."""
        # Simplified gradient computation using finite differences
        epsilon = 0.001
        gradient = np.zeros_like(input_sample)
        
        for i in range(len(input_sample)):
            input_plus = input_sample.copy()
            input_plus[i] += epsilon
            value_plus = self._compute_decision_value(qsvm_model, input_plus)
            
            input_minus = input_sample.copy()
            input_minus[i] -= epsilon
            value_minus = self._compute_decision_value(qsvm_model, input_minus)
            
            gradient[i] = (value_plus - value_minus) / (2 * epsilon)
        
        return gradient
    
    def _predict_qsvm(self, qsvm_model: 'QSVC', input_sample: np.ndarray) -> int:
        """Get QSVM prediction for input."""
        return np.random.randint(0, 2)  # Placeholder
    
    def _predict_classical_svm(self, svm_model: 'SVC', input_sample: np.ndarray) -> int:
        """Get classical SVM prediction."""
        return np.random.randint(0, 2)  # Placeholder
    
    def _compute_decision_value(self, qsvm_model: 'QSVC', input_sample: np.ndarray) -> float:
        """Compute distance to decision hyperplane."""
        return np.random.randn()  # Placeholder
    
    def _compute_quantum_feature_noise(self, sample: np.ndarray) -> np.ndarray:
        """Compute strategic noise in quantum feature space."""
        return 0.05 * np.random.randn(*sample.shape)
    
    def create_zz_feature_map(self, n_layers: int = 2) -> QuantumCircuit:
        """Create ZZ feature map for QSVM."""
        qr = QuantumRegister(self.n_qubits, 'q')
        feature_map = QuantumCircuit(qr)
        
        # Create parameterized ZZ feature map
        params = ParameterVector('x', length=self.n_qubits)
        
        for layer in range(n_layers):
            # Hadamard layer
            for qubit in range(self.n_qubits):
                feature_map.h(qubit)
            
            # Data encoding layer
            for qubit in range(self.n_qubits):
                feature_map.p(2 * params[qubit], qubit)
            
            # Entanglement layer (ZZ interactions)
            for qubit in range(self.n_qubits - 1):
                feature_map.cx(qubit, qubit + 1)
                feature_map.p(2 * (np.pi - params[qubit]) * (np.pi - params[qubit + 1]), qubit + 1)
                feature_map.cx(qubit, qubit + 1)
        
        return feature_map
    
    def get_attack_summary(self) -> Dict:
        """Get summary of all attacks performed."""
        if not self.attack_history:
            return {"message": "No attacks performed yet"}
        
        attack_types = [a['type'] for a in self.attack_history]
        
        summary = {
            'total_attacks': len(self.attack_history),
            'attack_types': {t: attack_types.count(t) for t in set(attack_types)},
            'attacks': self.attack_history
        }
        
        return summary


def demonstrate_qsvm_exploits():
    """Demonstrate various QSVM exploits."""
    print("\n" + "="*70)
    print("Quantum SVM Exploit Demonstration")
    print("="*70)
    
    if not QISKIT_AVAILABLE:
        print("\n[!] Qiskit ML not available. Install: pip install qiskit-machine-learning")
        return
    
    # Initialize exploit framework
    exploiter = QSVMExploit(n_qubits=4, backend="qasm_simulator")
    
    # Generate dummy data
    n_train = 50
    n_test = 20
    n_features = 4
    
    X_train = np.random.randn(n_train, n_features)
    y_train = np.random.randint(0, 2, n_train)
    X_test = np.random.randn(n_test, n_features)
    y_test = np.random.randint(0, 2, n_test)
    
    print(f"\n[*] Dataset generated:")
    print(f"    Training: {X_train.shape}")
    print(f"    Testing: {X_test.shape}")
    
    # Create quantum feature map
    feature_map = exploiter.create_zz_feature_map(n_layers=2)
    print(f"\n[*] Created ZZ feature map: {feature_map.num_qubits} qubits, {feature_map.depth()} depth")
    
    # Placeholder models
    qsvm_model = None
    classical_svm = None
    
    # 1. Kernel Manipulation Attack
    manipulated_fm, metrics = exploiter.kernel_manipulation_attack(
        feature_map, X_train, y_train, manipulation_strength=0.3
    )
    
    # 2. Feature Space Poisoning
    X_poisoned, y_poisoned = exploiter.feature_space_poisoning(
        X_train, y_train, poison_fraction=0.15
    )
    
    # 3. Adversarial Example Attack
    if len(X_test) > 0:
        adversarial, success = exploiter.adversarial_example_attack(
            qsvm_model, X_test[0], y_test[0], epsilon=0.1
        )
    
    # 4. Decision Boundary Attack
    boundary_metrics = exploiter.decision_boundary_attack(
        qsvm_model, X_test, y_test, boundary_margin=0.05
    )
    
    # 5. Kernel Matrix Backdoor
    trigger = np.array([0.5, 0.5, 0.5, 0.5])
    backdoored_fm = exploiter.kernel_matrix_backdoor(feature_map, trigger, target_label=1)
    
    # 6. Quantum Advantage Attack
    advantage_metrics = exploiter.quantum_advantage_attack(
        qsvm_model, classical_svm, X_test, y_test
    )
    
    # Print summary
    print("\n" + "="*70)
    print("Exploit Summary")
    print("="*70)
    summary = exploiter.get_attack_summary()
    print(f"Total exploits performed: {summary['total_attacks']}")
    print(f"Exploit type distribution: {summary['attack_types']}")
    
    print("\n[*] QSVM exploit demonstration complete!")
    
    return exploiter


if __name__ == "__main__":
    demonstrate_qsvm_exploits()
