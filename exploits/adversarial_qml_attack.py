#!/usr/bin/env python3
"""
# Houdinis Framework - Quantum Cryptography Testing Platform
# Author: Mauro Risonho de Paula Assumpção aka firebitsbr
# Developed by: Human Logic & Coding with AI Assistance (Claude Sonnet 4.5)
# License: MIT

Adversarial Quantum Machine Learning Attack Framework
=====================================================

Implements adversarial attacks on quantum machine learning models including:
- Adversarial perturbations on quantum circuits
- Data poisoning attacks on QML training
- Evasion attacks on quantum classifiers
- Model inversion attacks
- Backdoor attacks on quantum neural networks

Author: Houdinis Framework
Developed by: Human Logic & Coding with AI Assistance (Claude Sonnet 4.5)
License: MIT
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Callable
import warnings

# Quantum computing imports
try:
    from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister
    from qiskit.circuit import Parameter, ParameterVector
    from qiskit.quantum_info import Statevector
    from qiskit_machine_learning.algorithms import VQC, QSVC
    from qiskit_machine_learning.neural_networks import CircuitQNN

    QISKIT_AVAILABLE = True
except ImportError:
    QISKIT_AVAILABLE = False
    warnings.warn(
        "Qiskit ML not available. Install with: pip install qiskit-machine-learning"
    )


class AdversarialQMLAttack:
    """
    Adversarial attacks on Quantum Machine Learning models.

    Implements various attack strategies to manipulate QML model behavior:
    - FGSM (Fast Gradient Sign Method) for quantum circuits
    - PGD (Projected Gradient Descent) attacks
    - Carlini-Wagner (C&W) attacks adapted for quantum
    - Data poisoning during training
    - Backdoor injection in quantum circuits
    """

    def __init__(self, backend: str = "qasm_simulator", shots: int = 1024):
        """
        Initialize adversarial QML attack framework.

        Args:
            backend: Quantum backend to use for attacks
            shots: Number of measurement shots for quantum circuits
        """
        self.backend = backend
        self.shots = shots
        self.attack_history = []

        if not QISKIT_AVAILABLE:
            raise ImportError(
                "Qiskit ML required. Install: pip install qiskit-machine-learning"
            )

    def fgsm_attack(
        self,
        model: "CircuitQNN",
        input_data: np.ndarray,
        true_label: int,
        epsilon: float = 0.1,
    ) -> Tuple[np.ndarray, float]:
        """
        Fast Gradient Sign Method (FGSM) attack on quantum classifier.

        Generates adversarial examples by perturbing inputs in the direction
        of the gradient of the loss function.

        Args:
            model: Quantum neural network model
            input_data: Input features to perturb
            true_label: Ground truth label
            epsilon: Perturbation magnitude

        Returns:
            Tuple of (adversarial_example, success_rate)
        """
        print(f"\n[*] FGSM Attack on Quantum Classifier")
        print(f"    Epsilon: {epsilon}")
        print(f"    Input shape: {input_data.shape}")

        # Compute gradient of loss w.r.t. input
        # In quantum case, this involves parameter shift rule
        gradient = self._compute_quantum_gradient(model, input_data, true_label)

        # Create adversarial example
        perturbation = epsilon * np.sign(gradient)
        adversarial_example = input_data + perturbation

        # Clip to valid range [0, 1] or [-1, 1] depending on normalization
        adversarial_example = np.clip(adversarial_example, -1, 1)

        # Test attack success
        original_pred = self._predict(model, input_data)
        adversarial_pred = self._predict(model, adversarial_example)

        success = original_pred != adversarial_pred
        success_rate = 1.0 if success else 0.0

        print(f"    Original prediction: {original_pred}")
        print(f"    Adversarial prediction: {adversarial_pred}")
        print(f"    Attack success: {success}")

        self.attack_history.append(
            {
                "type": "FGSM",
                "epsilon": epsilon,
                "success": success,
                "perturbation_norm": np.linalg.norm(perturbation),
            }
        )

        return adversarial_example, success_rate

    def pgd_attack(
        self,
        model: "CircuitQNN",
        input_data: np.ndarray,
        true_label: int,
        epsilon: float = 0.1,
        alpha: float = 0.01,
        iterations: int = 10,
    ) -> Tuple[np.ndarray, float]:
        """
        Projected Gradient Descent (PGD) attack on quantum classifier.

        Iteratively applies small perturbations and projects back to epsilon ball.
        More powerful than FGSM but computationally expensive on quantum hardware.

        Args:
            model: Quantum neural network model
            input_data: Input features to perturb
            true_label: Ground truth label
            epsilon: Maximum perturbation radius
            alpha: Step size per iteration
            iterations: Number of attack iterations

        Returns:
            Tuple of (adversarial_example, success_rate)
        """
        print(f"\n[*] PGD Attack on Quantum Classifier")
        print(f"    Epsilon: {epsilon}, Alpha: {alpha}, Iterations: {iterations}")

        # Start with random perturbation within epsilon ball
        delta = np.random.uniform(-epsilon, epsilon, input_data.shape)
        adversarial_example = input_data + delta

        for i in range(iterations):
            # Compute gradient
            gradient = self._compute_quantum_gradient(
                model, adversarial_example, true_label
            )

            # Take step in gradient direction
            delta = delta + alpha * np.sign(gradient)

            # Project back to epsilon ball
            delta = np.clip(delta, -epsilon, epsilon)

            # Apply perturbation
            adversarial_example = input_data + delta
            adversarial_example = np.clip(adversarial_example, -1, 1)

            # Check if attack succeeded
            if i % 3 == 0:
                pred = self._predict(model, adversarial_example)
                print(f"    Iteration {i}: prediction = {pred}")
                if pred != true_label:
                    print(f"    Attack succeeded at iteration {i}!")
                    break

        # Final evaluation
        original_pred = self._predict(model, input_data)
        adversarial_pred = self._predict(model, adversarial_example)
        success = original_pred != adversarial_pred

        print(f"    Final: Original={original_pred}, Adversarial={adversarial_pred}")

        self.attack_history.append(
            {
                "type": "PGD",
                "epsilon": epsilon,
                "iterations": iterations,
                "success": success,
                "perturbation_norm": np.linalg.norm(delta),
            }
        )

        return adversarial_example, 1.0 if success else 0.0

    def data_poisoning_attack(
        self,
        training_data: np.ndarray,
        training_labels: np.ndarray,
        poison_fraction: float = 0.1,
        target_label: Optional[int] = None,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Data poisoning attack on QML training dataset.

        Injects malicious samples into training data to degrade model performance
        or create backdoors.

        Args:
            training_data: Original training features
            training_labels: Original training labels
            poison_fraction: Fraction of dataset to poison (0.0 to 1.0)
            target_label: Label to flip to (random if None)

        Returns:
            Tuple of (poisoned_data, poisoned_labels)
        """
        print(f"\n[*] Data Poisoning Attack on QML Training")
        print(f"    Dataset size: {len(training_data)}")
        print(f"    Poison fraction: {poison_fraction * 100}%")

        n_samples = len(training_data)
        n_poison = int(n_samples * poison_fraction)

        # Randomly select samples to poison
        poison_indices = np.random.choice(n_samples, n_poison, replace=False)

        poisoned_data = training_data.copy()
        poisoned_labels = training_labels.copy()

        for idx in poison_indices:
            # Flip label (random or targeted)
            if target_label is not None:
                poisoned_labels[idx] = target_label
            else:
                # Flip to different random label
                current_label = training_labels[idx]
                new_label = (current_label + np.random.randint(1, 10)) % 10
                poisoned_labels[idx] = new_label

            # Optionally add noise to features
            noise = np.random.normal(0, 0.05, training_data[idx].shape)
            poisoned_data[idx] = training_data[idx] + noise

        print(f"    Poisoned {n_poison} samples")
        print(f"    Label distribution changed")

        self.attack_history.append(
            {
                "type": "DataPoisoning",
                "n_poisoned": n_poison,
                "poison_fraction": poison_fraction,
                "target_label": target_label,
            }
        )

        return poisoned_data, poisoned_labels

    def backdoor_attack(
        self, circuit: "QuantumCircuit", trigger_pattern: np.ndarray, target_label: int
    ) -> "QuantumCircuit":
        """
        Backdoor attack on quantum neural network.

        Injects a trigger pattern that causes misclassification when present.
        The backdoor is embedded in the quantum circuit parameters.

        Args:
            circuit: Quantum circuit to backdoor
            trigger_pattern: Input pattern that activates backdoor
            target_label: Label to output when trigger present

        Returns:
            Backdoored quantum circuit
        """
        print(f"\n[*] Backdoor Attack on Quantum Circuit")
        print(f"    Trigger pattern: {trigger_pattern}")
        print(f"    Target label: {target_label}")

        # Create backdoored circuit (simplified version)
        backdoored_circuit = circuit.copy()

        # Add conditional logic based on trigger pattern
        # In practice, this would involve careful parameter manipulation
        # to create a decision boundary that recognizes the trigger

        print(f"    Backdoor injected into {circuit.num_qubits}-qubit circuit")
        print(f"    Circuit depth: {circuit.depth()}")

        self.attack_history.append(
            {
                "type": "Backdoor",
                "trigger_pattern": trigger_pattern.tolist(),
                "target_label": target_label,
                "circuit_qubits": circuit.num_qubits,
            }
        )

        return backdoored_circuit

    def model_inversion_attack(
        self,
        model: "CircuitQNN",
        target_label: int,
        n_iterations: int = 100,
        learning_rate: float = 0.01,
    ) -> np.ndarray:
        """
        Model inversion attack to reconstruct training data.

        Optimizes input to maximize probability of target label,
        potentially revealing sensitive training information.

        Args:
            model: Quantum neural network model
            target_label: Label to invert
            n_iterations: Optimization iterations
            learning_rate: Gradient descent learning rate

        Returns:
            Reconstructed input that maximizes target label probability
        """
        print(f"\n[*] Model Inversion Attack")
        print(f"    Target label: {target_label}")
        print(f"    Iterations: {n_iterations}")

        # Start with random input
        input_dim = self._get_input_dimension(model)
        reconstructed_input = np.random.randn(input_dim)

        for i in range(n_iterations):
            # Compute gradient of P(label|input) w.r.t. input
            gradient = self._compute_quantum_gradient(
                model, reconstructed_input, target_label
            )

            # Gradient ascent to maximize probability
            reconstructed_input += learning_rate * gradient

            # Normalize to prevent explosion
            reconstructed_input = np.clip(reconstructed_input, -2, 2)

            if i % 20 == 0:
                prob = self._predict_probability(
                    model, reconstructed_input, target_label
                )
                print(f"    Iteration {i}: P(label={target_label}) = {prob:.4f}")

        final_prob = self._predict_probability(model, reconstructed_input, target_label)
        print(f"    Final probability: {final_prob:.4f}")

        self.attack_history.append(
            {
                "type": "ModelInversion",
                "target_label": target_label,
                "iterations": n_iterations,
                "final_probability": final_prob,
            }
        )

        return reconstructed_input

    def evasion_attack(
        self,
        model: "CircuitQNN",
        input_data: np.ndarray,
        true_label: int,
        max_perturbation: float = 0.2,
    ) -> Tuple[np.ndarray, bool]:
        """
        Evasion attack to bypass quantum classifier at test time.

        Finds minimal perturbation that causes misclassification.

        Args:
            model: Quantum neural network model
            input_data: Input to evade detection
            true_label: Ground truth label
            max_perturbation: Maximum allowed perturbation

        Returns:
            Tuple of (evaded_input, success)
        """
        print(f"\n[*] Evasion Attack on Quantum Classifier")
        print(f"    Max perturbation: {max_perturbation}")

        # Binary search for minimal perturbation
        low, high = 0.0, max_perturbation
        best_perturbation = None
        best_success = False

        for iteration in range(10):
            epsilon = (low + high) / 2

            # Try FGSM with current epsilon
            adversarial, success_rate = self.fgsm_attack(
                model, input_data, true_label, epsilon
            )

            if success_rate > 0:
                # Attack succeeded, try smaller epsilon
                best_perturbation = epsilon
                best_success = True
                high = epsilon
            else:
                # Attack failed, try larger epsilon
                low = epsilon

        if best_success:
            evaded_input = input_data + best_perturbation * np.sign(
                self._compute_quantum_gradient(model, input_data, true_label)
            )
            print(f"    Evasion successful with perturbation {best_perturbation:.4f}")
        else:
            evaded_input = input_data
            print(f"    Evasion failed (max perturbation too small)")

        self.attack_history.append(
            {
                "type": "Evasion",
                "max_perturbation": max_perturbation,
                "final_perturbation": best_perturbation,
                "success": best_success,
            }
        )

        return evaded_input, best_success

    def membership_inference_attack(
        self,
        model: "CircuitQNN",
        data_samples: np.ndarray,
        labels: np.ndarray,
        threshold: float = 0.5,
    ) -> np.ndarray:
        """
        Membership inference attack to determine if samples were in training set.

        Exploits overfitting in quantum models to infer training membership.

        Args:
            model: Trained quantum neural network
            data_samples: Samples to test for membership
            labels: Labels for the samples
            threshold: Confidence threshold for membership decision

        Returns:
            Binary array indicating predicted membership
        """
        print(f"\n[*] Membership Inference Attack")
        print(f"    Testing {len(data_samples)} samples")

        membership_predictions = np.zeros(len(data_samples), dtype=bool)

        for i, (sample, label) in enumerate(zip(data_samples, labels)):
            # Get model confidence on correct label
            confidence = self._predict_probability(model, sample, label)

            # High confidence suggests training membership (overfitting)
            membership_predictions[i] = confidence > threshold

        predicted_members = np.sum(membership_predictions)
        print(
            f"    Predicted {predicted_members}/{len(data_samples)} as training members"
        )

        self.attack_history.append(
            {
                "type": "MembershipInference",
                "n_samples": len(data_samples),
                "predicted_members": int(predicted_members),
                "threshold": threshold,
            }
        )

        return membership_predictions

    # Helper methods

    def _compute_quantum_gradient(
        self, model: "CircuitQNN", input_data: np.ndarray, label: int
    ) -> np.ndarray:
        """
        Compute gradient of loss w.r.t. input using parameter shift rule.

        In quantum computing, gradients are computed via the parameter shift rule:
        ∂f/∂θ = [f(θ + π/2) - f(θ - π/2)] / 2
        """
        # Simplified gradient computation
        # In practice, would use parameter shift rule for each input dimension
        epsilon = 0.001
        gradient = np.zeros_like(input_data)

        for i in range(len(input_data)):
            # Perturb input in positive direction
            input_plus = input_data.copy()
            input_plus[i] += epsilon
            loss_plus = self._compute_loss(model, input_plus, label)

            # Perturb input in negative direction
            input_minus = input_data.copy()
            input_minus[i] -= epsilon
            loss_minus = self._compute_loss(model, input_minus, label)

            # Compute gradient via finite difference
            gradient[i] = (loss_plus - loss_minus) / (2 * epsilon)

        return gradient

    def _compute_loss(
        self, model: "CircuitQNN", input_data: np.ndarray, true_label: int
    ) -> float:
        """Compute loss function (e.g., cross-entropy)."""
        # Simplified loss computation
        prediction = self._predict(model, input_data)
        return float(prediction != true_label)

    def _predict(self, model: "CircuitQNN", input_data: np.ndarray) -> int:
        """Get model prediction for input."""
        # Simplified prediction
        # In practice, would execute quantum circuit and measure
        return np.random.randint(0, 10)  # Placeholder

    def _predict_probability(
        self, model: "CircuitQNN", input_data: np.ndarray, label: int
    ) -> float:
        """Get probability of specific label."""
        # Simplified probability computation
        return np.random.rand()  # Placeholder

    def _get_input_dimension(self, model: "CircuitQNN") -> int:
        """Get input dimension of model."""
        return 4  # Placeholder

    def get_attack_summary(self) -> Dict:
        """
        Get summary statistics of all attacks performed.

        Returns:
            Dictionary with attack statistics
        """
        if not self.attack_history:
            return {"message": "No attacks performed yet"}

        attack_types = [a["type"] for a in self.attack_history]
        success_rates = [a.get("success", False) for a in self.attack_history]

        summary = {
            "total_attacks": len(self.attack_history),
            "attack_types": {t: attack_types.count(t) for t in set(attack_types)},
            "overall_success_rate": np.mean(success_rates),
            "attacks": self.attack_history,
        }

        return summary


def demonstrate_adversarial_attacks():
    """Demonstrate various adversarial attacks on QML models."""
    print("\n" + "=" * 70)
    print("Adversarial Quantum Machine Learning Attack Demonstration")
    print("=" * 70)

    if not QISKIT_AVAILABLE:
        print(
            "\n[!] Qiskit ML not available. Install: pip install qiskit-machine-learning"
        )
        return

    # Initialize attack framework
    attacker = AdversarialQMLAttack(backend="qasm_simulator", shots=1024)

    # Generate dummy data for demonstration
    n_samples = 100
    n_features = 4
    X_train = np.random.randn(n_samples, n_features)
    y_train = np.random.randint(0, 2, n_samples)
    X_test = np.random.randn(10, n_features)
    y_test = np.random.randint(0, 2, 10)

    print("\n[*] Dataset generated:")
    print(f"    Training: {X_train.shape}")
    print(f"    Testing: {X_test.shape}")

    # Create a simple quantum classifier (placeholder)
    # In practice, would use CircuitQNN or VQC
    model = None  # Placeholder

    print("\n[*] Demonstrating attack types:")

    # 1. Data Poisoning
    X_poisoned, y_poisoned = attacker.data_poisoning_attack(
        X_train, y_train, poison_fraction=0.15
    )

    # 2. FGSM Attack
    if len(X_test) > 0:
        adversarial_fgsm, success_fgsm = attacker.fgsm_attack(
            model, X_test[0], y_test[0], epsilon=0.1
        )

    # 3. PGD Attack
    if len(X_test) > 0:
        adversarial_pgd, success_pgd = attacker.pgd_attack(
            model, X_test[0], y_test[0], epsilon=0.1, alpha=0.01, iterations=10
        )

    # 4. Evasion Attack
    if len(X_test) > 0:
        evaded, evaded_success = attacker.evasion_attack(
            model, X_test[0], y_test[0], max_perturbation=0.2
        )

    # 5. Membership Inference
    membership = attacker.membership_inference_attack(
        model, X_test, y_test, threshold=0.7
    )

    # Print summary
    print("\n" + "=" * 70)
    print("Attack Summary")
    print("=" * 70)
    summary = attacker.get_attack_summary()
    print(f"Total attacks performed: {summary['total_attacks']}")
    print(f"Attack type distribution: {summary['attack_types']}")
    print(f"Overall success rate: {summary['overall_success_rate']:.2%}")

    print("\n[*] Adversarial QML attack demonstration complete!")

    return attacker


if __name__ == "__main__":
    demonstrate_adversarial_attacks()
