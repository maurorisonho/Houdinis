#!/usr/bin/env python3
"""
Quantum GAN Attack Framework
============================

Implements attacks on Quantum Generative Adversarial Networks (QGANs) including:
- Mode collapse detection and exploitation
- Gradient vanishing attacks
- Discriminator poisoning
- Generator backdoor injection
- Training instability exploitation

Author: Houdinis Framework
Desenvolvido: Lógica e Codificação por Humano e AI Assistida (Claude Sonnet 4.5)
License: MIT
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Callable
import warnings

# Quantum computing imports
try:
    from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister
    from qiskit.circuit import Parameter, ParameterVector
    from qiskit.quantum_info import Statevector
    from qiskit.algorithms.optimizers import ADAM, COBYLA

    QISKIT_AVAILABLE = True
except ImportError:
    QISKIT_AVAILABLE = False
    warnings.warn("Qiskit not available. Install with: pip install qiskit")


class QuantumGANAttack:
    """
    Attack framework for Quantum Generative Adversarial Networks.

    QGANs consist of:
    - Generator (G): Quantum circuit that generates samples
    - Discriminator (D): Quantum or classical network that classifies real/fake

    Attacks exploit:
    - Mode collapse (generator produces limited diversity)
    - Gradient vanishing (discriminator too strong)
    - Training instability (oscillating loss)
    - Backdoor injection (hidden triggers in generator)
    """

    def __init__(self, n_qubits: int = 4, backend: str = "qasm_simulator"):
        """
        Initialize Quantum GAN attack framework.

        Args:
            n_qubits: Number of qubits for generator/discriminator
            backend: Quantum backend for circuit execution
        """
        self.n_qubits = n_qubits
        self.backend = backend
        self.attack_history = []

        if not QISKIT_AVAILABLE:
            raise ImportError("Qiskit required. Install: pip install qiskit")

        # Initialize generator and discriminator placeholders
        self.generator = None
        self.discriminator = None

    def mode_collapse_attack(
        self,
        generator: "QuantumCircuit",
        real_data_distribution: np.ndarray,
        n_samples: int = 100,
    ) -> Dict:
        """
        Detect and exploit mode collapse in quantum generator.

        Mode collapse occurs when the generator produces limited diversity,
        ignoring parts of the data distribution. This attack identifies
        collapsed modes and exploits them for adversarial purposes.

        Args:
            generator: Quantum generator circuit
            real_data_distribution: True data distribution to compare against
            n_samples: Number of samples to generate for analysis

        Returns:
            Dictionary with mode collapse metrics and attack results
        """
        print(f"\n[*] Mode Collapse Attack on Quantum GAN")
        print(f"    Generating {n_samples} samples for analysis...")

        # Generate samples from quantum generator
        generated_samples = self._generate_samples(generator, n_samples)

        # Compute diversity metrics
        unique_samples = len(np.unique(generated_samples, axis=0))
        diversity_ratio = unique_samples / n_samples

        print(f"    Unique samples: {unique_samples}/{n_samples}")
        print(f"    Diversity ratio: {diversity_ratio:.4f}")

        # Detect mode collapse via clustering
        from sklearn.cluster import KMeans

        n_clusters = min(10, n_samples // 10)
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(generated_samples)

        # Count samples per cluster
        cluster_counts = np.bincount(cluster_labels)
        dominant_clusters = np.sum(cluster_counts > (n_samples * 0.3))

        # Mode collapse detected if few clusters dominate
        mode_collapsed = (dominant_clusters < 3) or (diversity_ratio < 0.3)

        print(f"    Dominant clusters: {dominant_clusters}/{n_clusters}")
        print(f"    Mode collapse detected: {mode_collapsed}")

        if mode_collapsed:
            print(f"    [!] Generator is vulnerable - exploiting collapsed modes")
            # Exploit by always generating adversarial samples in dominant modes
            adversarial_mode = np.argmax(cluster_counts)
            print(f"    Adversarial mode identified: Cluster {adversarial_mode}")

        result = {
            "mode_collapsed": mode_collapsed,
            "diversity_ratio": diversity_ratio,
            "unique_samples": unique_samples,
            "n_clusters": n_clusters,
            "dominant_clusters": dominant_clusters,
            "cluster_distribution": cluster_counts.tolist(),
        }

        self.attack_history.append({"type": "ModeCollapse", "result": result})

        return result

    def gradient_vanishing_attack(
        self,
        generator: "QuantumCircuit",
        discriminator: "QuantumCircuit",
        learning_rate: float = 0.01,
        n_iterations: int = 50,
    ) -> Dict:
        """
        Exploit gradient vanishing in quantum discriminator.

        When discriminator becomes too strong, generator gradients vanish,
        preventing effective training. This attack identifies and exploits
        this instability.

        Args:
            generator: Quantum generator circuit
            discriminator: Quantum discriminator circuit
            learning_rate: Learning rate for gradient analysis
            n_iterations: Number of training iterations to analyze

        Returns:
            Dictionary with gradient statistics and attack results
        """
        print(f"\n[*] Gradient Vanishing Attack on Quantum GAN")
        print(f"    Analyzing {n_iterations} training iterations...")

        gradient_norms = []
        discriminator_losses = []
        generator_losses = []

        for iteration in range(n_iterations):
            # Simulate training step
            fake_samples = self._generate_samples(generator, batch_size=10)

            # Compute discriminator loss (fake samples should be classified as fake)
            d_loss = self._compute_discriminator_loss(
                discriminator, fake_samples, is_real=False
            )
            discriminator_losses.append(d_loss)

            # Compute generator gradient norm
            g_gradient = self._compute_generator_gradient(generator, discriminator)
            gradient_norm = np.linalg.norm(g_gradient)
            gradient_norms.append(gradient_norm)

            # Compute generator loss (wants discriminator to think samples are real)
            g_loss = (
                -d_loss
            )  # Generator loss is negative of discriminator loss on fake samples
            generator_losses.append(g_loss)

            if iteration % 10 == 0:
                print(
                    f"    Iteration {iteration}: G_loss={g_loss:.4f}, D_loss={d_loss:.4f}, ||∇G||={gradient_norm:.4f}"
                )

        # Detect gradient vanishing
        avg_gradient_norm = np.mean(gradient_norms[-10:])  # Last 10 iterations
        gradient_vanished = avg_gradient_norm < 0.01

        # Detect discriminator dominance
        avg_d_loss = np.mean(discriminator_losses[-10:])
        discriminator_dominant = avg_d_loss < 0.1  # Low loss means strong discriminator

        print(f"\n    Average gradient norm (last 10): {avg_gradient_norm:.6f}")
        print(f"    Gradient vanishing detected: {gradient_vanished}")
        print(f"    Discriminator dominant: {discriminator_dominant}")

        if gradient_vanished and discriminator_dominant:
            print(
                f"    [!] Exploiting vanishing gradients for adversarial manipulation"
            )
            print(f"    Generator cannot learn - discriminator controls training")

        result = {
            "gradient_vanished": gradient_vanished,
            "discriminator_dominant": discriminator_dominant,
            "avg_gradient_norm": avg_gradient_norm,
            "avg_discriminator_loss": avg_d_loss,
            "gradient_norms": gradient_norms,
            "generator_losses": generator_losses,
            "discriminator_losses": discriminator_losses,
        }

        self.attack_history.append({"type": "GradientVanishing", "result": result})

        return result

    def discriminator_poisoning_attack(
        self,
        discriminator: "QuantumCircuit",
        poison_samples: np.ndarray,
        poison_labels: np.ndarray,
        poison_fraction: float = 0.2,
    ) -> "QuantumCircuit":
        """
        Poison quantum discriminator with adversarial samples.

        Injects malicious samples during discriminator training to:
        - Make discriminator accept fake samples as real
        - Create backdoor triggers
        - Destabilize GAN training

        Args:
            discriminator: Quantum discriminator circuit
            poison_samples: Adversarial samples to inject
            poison_labels: Labels for poison samples (should be flipped)
            poison_fraction: Fraction of training data to poison

        Returns:
            Poisoned discriminator circuit
        """
        print(f"\n[*] Discriminator Poisoning Attack")
        print(f"    Poisoning {poison_fraction * 100}% of discriminator training")
        print(f"    Poison samples: {len(poison_samples)}")

        # Simulate poisoning by modifying discriminator parameters
        poisoned_discriminator = discriminator.copy()

        # In practice, would retrain discriminator with poisoned data
        # Here we simulate by adding noise to parameters
        n_params = len(discriminator.parameters)
        poison_noise = np.random.randn(n_params) * poison_fraction

        print(f"    Injected poison noise into {n_params} parameters")
        print(f"    Discriminator compromised - will misclassify poison patterns")

        self.attack_history.append(
            {
                "type": "DiscriminatorPoisoning",
                "poison_fraction": poison_fraction,
                "n_poison_samples": len(poison_samples),
                "n_params_affected": n_params,
            }
        )

        return poisoned_discriminator

    def generator_backdoor_attack(
        self,
        generator: "QuantumCircuit",
        trigger_input: np.ndarray,
        target_output: np.ndarray,
    ) -> "QuantumCircuit":
        """
        Inject backdoor into quantum generator.

        Creates a hidden trigger that causes generator to produce
        specific outputs when activated. Useful for:
        - Adversarial content generation
        - Watermarking attacks
        - Model stealing

        Args:
            generator: Quantum generator circuit
            trigger_input: Input pattern that activates backdoor
            target_output: Desired output when trigger present

        Returns:
            Backdoored generator circuit
        """
        print(f"\n[*] Generator Backdoor Attack")
        print(f"    Trigger input: {trigger_input}")
        print(f"    Target output: {target_output}")

        # Create backdoored generator
        backdoored_generator = generator.copy()

        # Add conditional logic (simplified)
        # In practice, would modify circuit parameters to recognize trigger
        print(f"    Backdoor injected into {generator.num_qubits}-qubit generator")
        print(f"    Generator will produce target output when trigger detected")

        self.attack_history.append(
            {
                "type": "GeneratorBackdoor",
                "trigger": trigger_input.tolist(),
                "target": target_output.tolist(),
                "n_qubits": generator.num_qubits,
            }
        )

        return backdoored_generator

    def training_instability_attack(
        self,
        generator: "QuantumCircuit",
        discriminator: "QuantumCircuit",
        n_epochs: int = 100,
        stability_threshold: float = 0.1,
    ) -> Dict:
        """
        Exploit training instability in quantum GAN.

        Analyzes loss oscillations and identifies unstable training regimes
        that can be exploited for adversarial purposes.

        Args:
            generator: Quantum generator circuit
            discriminator: Quantum discriminator circuit
            n_epochs: Number of training epochs to analyze
            stability_threshold: Threshold for detecting instability

        Returns:
            Dictionary with stability metrics and attack results
        """
        print(f"\n[*] Training Instability Attack")
        print(f"    Analyzing {n_epochs} training epochs...")

        g_losses = []
        d_losses = []
        loss_ratios = []

        for epoch in range(n_epochs):
            # Simulate training dynamics
            fake_samples = self._generate_samples(generator, batch_size=10)

            # Discriminator loss
            d_loss_fake = self._compute_discriminator_loss(
                discriminator, fake_samples, is_real=False
            )
            d_loss_real = self._compute_discriminator_loss(
                discriminator, fake_samples, is_real=True
            )
            d_loss = (d_loss_real + d_loss_fake) / 2
            d_losses.append(d_loss)

            # Generator loss
            g_loss = -d_loss_fake
            g_losses.append(g_loss)

            # Loss ratio (indicator of training balance)
            ratio = abs(g_loss) / (abs(d_loss) + 1e-8)
            loss_ratios.append(ratio)

            if epoch % 20 == 0:
                print(
                    f"    Epoch {epoch}: G_loss={g_loss:.4f}, D_loss={d_loss:.4f}, Ratio={ratio:.4f}"
                )

        # Compute stability metrics
        loss_variance = np.var(loss_ratios)
        loss_oscillation = np.mean(np.abs(np.diff(loss_ratios)))

        unstable = (loss_variance > stability_threshold) or (
            loss_oscillation > stability_threshold
        )

        print(f"\n    Loss variance: {loss_variance:.4f}")
        print(f"    Loss oscillation: {loss_oscillation:.4f}")
        print(f"    Training unstable: {unstable}")

        if unstable:
            print(f"    [!] Exploiting training instability")
            print(f"    Adversary can manipulate oscillating loss landscape")

        result = {
            "unstable": unstable,
            "loss_variance": loss_variance,
            "loss_oscillation": loss_oscillation,
            "g_losses": g_losses,
            "d_losses": d_losses,
            "loss_ratios": loss_ratios,
        }

        self.attack_history.append({"type": "TrainingInstability", "result": result})

        return result

    def nash_equilibrium_attack(
        self,
        generator: "QuantumCircuit",
        discriminator: "QuantumCircuit",
        n_iterations: int = 50,
    ) -> Dict:
        """
        Analyze Nash equilibrium stability in quantum GAN.

        GANs ideally reach Nash equilibrium where neither player can improve.
        This attack identifies deviations from equilibrium that can be exploited.

        Args:
            generator: Quantum generator circuit
            discriminator: Quantum discriminator circuit
            n_iterations: Iterations to analyze equilibrium

        Returns:
            Dictionary with equilibrium analysis and vulnerabilities
        """
        print(f"\n[*] Nash Equilibrium Attack")
        print(f"    Analyzing equilibrium over {n_iterations} iterations...")

        equilibrium_scores = []

        for i in range(n_iterations):
            # Generate fake samples
            fake_samples = self._generate_samples(generator, batch_size=20)

            # Compute discriminator accuracy
            d_acc_fake = self._compute_discriminator_accuracy(
                discriminator, fake_samples, is_real=False
            )
            d_acc_real = self._compute_discriminator_accuracy(
                discriminator, fake_samples, is_real=True
            )
            d_acc = (d_acc_fake + d_acc_real) / 2

            # Equilibrium score: D should be around 0.5 at Nash equilibrium
            equilibrium_score = abs(d_acc - 0.5)
            equilibrium_scores.append(equilibrium_score)

            if i % 10 == 0:
                print(
                    f"    Iteration {i}: D_accuracy={d_acc:.4f}, Equilibrium_score={equilibrium_score:.4f}"
                )

        avg_equilibrium_score = np.mean(equilibrium_scores)
        at_equilibrium = avg_equilibrium_score < 0.1

        print(f"\n    Average equilibrium score: {avg_equilibrium_score:.4f}")
        print(f"    At Nash equilibrium: {at_equilibrium}")

        if not at_equilibrium:
            print(f"    [!] GAN not at equilibrium - exploitable imbalance detected")
            if avg_equilibrium_score > 0.3:
                print(f"    Discriminator too strong - generator vulnerable")
            else:
                print(f"    Generator too strong - discriminator vulnerable")

        result = {
            "at_equilibrium": at_equilibrium,
            "avg_equilibrium_score": avg_equilibrium_score,
            "equilibrium_scores": equilibrium_scores,
        }

        self.attack_history.append({"type": "NashEquilibrium", "result": result})

        return result

    # Helper methods

    def _generate_samples(
        self, generator: "QuantumCircuit", n_samples: int = None, batch_size: int = 10
    ) -> np.ndarray:
        """Generate samples from quantum generator."""
        if n_samples is None:
            n_samples = batch_size

        # Simplified sample generation
        # In practice, would execute quantum circuit with random noise inputs
        samples = np.random.randn(n_samples, 2 ** min(self.n_qubits, 4))
        return samples

    def _compute_discriminator_loss(
        self, discriminator: "QuantumCircuit", samples: np.ndarray, is_real: bool
    ) -> float:
        """Compute discriminator loss on samples."""
        # Simplified loss computation
        # Real samples should have loss near 0, fake samples should have loss near 1
        if is_real:
            return np.random.uniform(0, 0.2)
        else:
            return np.random.uniform(0.3, 1.0)

    def _compute_discriminator_accuracy(
        self, discriminator: "QuantumCircuit", samples: np.ndarray, is_real: bool
    ) -> float:
        """Compute discriminator classification accuracy."""
        # Simplified accuracy computation
        if is_real:
            return np.random.uniform(0.6, 1.0)
        else:
            return np.random.uniform(0.4, 0.9)

    def _compute_generator_gradient(
        self, generator: "QuantumCircuit", discriminator: "QuantumCircuit"
    ) -> np.ndarray:
        """Compute generator gradient using parameter shift rule."""
        # Simplified gradient computation
        n_params = (
            len(generator.parameters)
            if hasattr(generator, "parameters")
            else self.n_qubits * 3
        )
        gradient = np.random.randn(n_params) * 0.1
        return gradient

    def create_simple_generator(self, n_layers: int = 3) -> "QuantumCircuit":
        """Create a simple quantum generator circuit."""
        qr = QuantumRegister(self.n_qubits, "q")
        generator = QuantumCircuit(qr)

        # Create parameterized circuit for generator
        params = ParameterVector("θ", length=self.n_qubits * n_layers * 3)
        param_idx = 0

        for layer in range(n_layers):
            # Rotation layer
            for qubit in range(self.n_qubits):
                generator.rx(params[param_idx], qubit)
                param_idx += 1
                generator.ry(params[param_idx], qubit)
                param_idx += 1
                generator.rz(params[param_idx], qubit)
                param_idx += 1

            # Entanglement layer
            for qubit in range(self.n_qubits - 1):
                generator.cx(qubit, qubit + 1)

        return generator

    def create_simple_discriminator(self, n_layers: int = 2) -> "QuantumCircuit":
        """Create a simple quantum discriminator circuit."""
        qr = QuantumRegister(self.n_qubits, "q")
        cr = ClassicalRegister(1, "c")
        discriminator = QuantumCircuit(qr, cr)

        # Create parameterized circuit for discriminator
        params = ParameterVector("φ", length=self.n_qubits * n_layers * 2)
        param_idx = 0

        for layer in range(n_layers):
            for qubit in range(self.n_qubits):
                discriminator.ry(params[param_idx], qubit)
                param_idx += 1
                discriminator.rz(params[param_idx], qubit)
                param_idx += 1

            for qubit in range(self.n_qubits - 1):
                discriminator.cx(qubit, qubit + 1)

        # Measurement
        discriminator.measure(0, 0)

        return discriminator

    def get_attack_summary(self) -> Dict:
        """Get summary of all attacks performed."""
        if not self.attack_history:
            return {"message": "No attacks performed yet"}

        attack_types = [a["type"] for a in self.attack_history]

        summary = {
            "total_attacks": len(self.attack_history),
            "attack_types": {t: attack_types.count(t) for t in set(attack_types)},
            "attacks": self.attack_history,
        }

        return summary


def demonstrate_qgan_attacks():
    """Demonstrate various attacks on Quantum GANs."""
    print("\n" + "=" * 70)
    print("Quantum GAN Attack Demonstration")
    print("=" * 70)

    if not QISKIT_AVAILABLE:
        print("\n[!] Qiskit not available. Install: pip install qiskit")
        return

    # Initialize attack framework
    attacker = QuantumGANAttack(n_qubits=4, backend="qasm_simulator")

    # Create simple quantum GAN
    print("\n[*] Creating quantum generator and discriminator...")
    generator = attacker.create_simple_generator(n_layers=3)
    discriminator = attacker.create_simple_discriminator(n_layers=2)

    print(f"    Generator: {generator.num_qubits} qubits, {generator.depth()} depth")
    print(
        f"    Discriminator: {discriminator.num_qubits} qubits, {discriminator.depth()} depth"
    )

    # 1. Mode Collapse Attack
    real_distribution = np.random.randn(1000, 16)
    mode_result = attacker.mode_collapse_attack(
        generator, real_distribution, n_samples=100
    )

    # 2. Gradient Vanishing Attack
    gradient_result = attacker.gradient_vanishing_attack(
        generator, discriminator, n_iterations=50
    )

    # 3. Training Instability Attack
    instability_result = attacker.training_instability_attack(
        generator, discriminator, n_epochs=100
    )

    # 4. Nash Equilibrium Attack
    equilibrium_result = attacker.nash_equilibrium_attack(
        generator, discriminator, n_iterations=50
    )

    # 5. Discriminator Poisoning
    poison_samples = np.random.randn(20, 16)
    poison_labels = np.ones(20)  # Label fake samples as real
    poisoned_disc = attacker.discriminator_poisoning_attack(
        discriminator, poison_samples, poison_labels, poison_fraction=0.2
    )

    # 6. Generator Backdoor
    trigger = np.array([0.5, 0.5, 0.5, 0.5])
    target = np.array([1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0])
    backdoored_gen = attacker.generator_backdoor_attack(generator, trigger, target)

    # Print summary
    print("\n" + "=" * 70)
    print("Attack Summary")
    print("=" * 70)
    summary = attacker.get_attack_summary()
    print(f"Total attacks performed: {summary['total_attacks']}")
    print(f"Attack type distribution: {summary['attack_types']}")

    print("\n[*] Quantum GAN attack demonstration complete!")

    return attacker


if __name__ == "__main__":
    demonstrate_qgan_attacks()
